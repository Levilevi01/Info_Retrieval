{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38d2ad7a",
   "metadata": {},
   "source": [
    "# Deliverable 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2fbece",
   "metadata": {},
   "source": [
    "SNumbers: u264332, u264443, u264202\n",
    "\n",
    "Names: Levente Olivér Bódi, Riccardo Zamuner, Giada Izzo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b3b603",
   "metadata": {},
   "source": [
    "## Previous deliverable code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd734c6",
   "metadata": {},
   "source": [
    "This is the same code of the previous deliverable minus prints and commentary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17d228d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/just_riccio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/just_riccio/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65b9f84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess a text by tokenizing, lowercasing, removing stop words, and stemming.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # Remove punctuation\n",
    "    tokens = [re.sub(r\"[^\\w\\s]\", \"\", token) for token in tokens]\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Remove punctuation\n",
    "    tokens = [re.sub(r\"[^\\w\\s]\", \"\", token) for token in tokens]\n",
    "\n",
    "    # Stem the tokens\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    # Remove empty strings\n",
    "    tokens = [token for token in tokens if token]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def clean_seller(text):\n",
    "    \"\"\"\n",
    "    Clean the seller field by removing unwanted trailing phrases.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove unwanted trailing phrases and everything after them\n",
    "    remove_phrases = [\n",
    "        \"Seller changed\",\n",
    "        \"(Not Enough Ratin\",\n",
    "        \"(New Sell\"\n",
    "    ]\n",
    "    for phrase in remove_phrases:\n",
    "        idx = text.find(phrase)\n",
    "        if idx != -1:\n",
    "            text = text[:idx]\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_non_textual(document):\n",
    "    \"\"\"\n",
    "    Preprocess non-textual fields in the document.\n",
    "    \"\"\"\n",
    "\n",
    "    # Discount preprocessing: convert from string \"xx% off\" to integer xx\n",
    "    # also taking into account documents without discount\n",
    "    if isinstance(document[\"discount\"], str) and \"%\" in document[\"discount\"]:\n",
    "        document[\"discount\"] = int(document[\"discount\"][:document[\"discount\"].find(\"%\")])\n",
    "    else:\n",
    "        document[\"discount\"] = 0\n",
    "        \n",
    "    # Merge all values from product_details dictionary and preprocess\n",
    "    if isinstance(document[\"product_details\"], dict):\n",
    "        details_text = \" \".join(str(v) for v in document[\"product_details\"].values())\n",
    "    elif isinstance(document[\"product_details\"], list):\n",
    "        # If it's a list of dicts, merge all values from all dicts\n",
    "        details_text = \" \".join(str(v) for d in document[\"product_details\"] if isinstance(d, dict) for v in d.values())\n",
    "    else:\n",
    "        details_text = str(document[\"product_details\"])\n",
    "    document[\"product_details\"] = preprocess_text(details_text)\n",
    "\n",
    "    # Convert actual_price and selling_price to integers (remove commas)\n",
    "    # If actual_price is NaN, set it to discounted selling_price\n",
    "    for price_field in [\"actual_price\", \"selling_price\"]:\n",
    "        if isinstance(document[price_field], str):\n",
    "            price_str = document[price_field].replace(\",\", \"\")\n",
    "            price_val = price_str.split(\".\")[0]\n",
    "            document[price_field] = int(price_val) if price_val.isdigit() else 0\n",
    "\n",
    "    # If actual_price is missing or zero, set it to discounted selling_price\n",
    "    if (\"actual_price\" not in document or document[\"actual_price\"] == 0) and \"selling_price\" in document:\n",
    "        document[\"actual_price\"] = int(int(document[\"selling_price\"])*document[\"discount\"]/100)\n",
    "\n",
    "    # Convert average_rating to float, set to NaN if missing or empty\n",
    "    if \"average_rating\" in document and str(document[\"average_rating\"]).strip() != \"\":\n",
    "        try:\n",
    "            document[\"average_rating\"] = float(document[\"average_rating\"])\n",
    "        except ValueError:\n",
    "            document[\"average_rating\"] = float(\"nan\")\n",
    "    else:\n",
    "        document[\"average_rating\"] = float(\"nan\")\n",
    "\n",
    "    return document\n",
    "\n",
    "def preprocess_document(document):\n",
    "    \"\"\"\n",
    "    Join all preprocessing steps for a document.\n",
    "    \"\"\"\n",
    "\n",
    "    document[\"description\"] = preprocess_text(document[\"description\"])\n",
    "    document[\"title\"] = preprocess_text(document[\"title\"])\n",
    "    document[\"seller\"] = clean_seller(document[\"seller\"])\n",
    "    document[\"brand\"] = document[\"brand\"].lower().split()\n",
    "\n",
    "    document = preprocess_non_textual(document)\n",
    "\n",
    "    return document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "977d4751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFY THIS PATH AS NEEDED\n",
    "file_path = \"../../data/fashion_products_dataset.json\"\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff0e1f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_actual_price(row):\n",
    "    # if actual_price is empty, try to compute it:\n",
    "    # either from selling_price and discount, or just use selling_price\n",
    "    if row['actual_price'] == '':\n",
    "        # Convert selling_price and discount to float if not empty\n",
    "        if row['selling_price'] != '' and row['discount'] != '':\n",
    "            selling_price = float(str(row['selling_price']).replace(',', ''))\n",
    "            discount = float(str(row['discount']).replace('%', '').replace('off', '').strip())\n",
    "            return selling_price * (1 - discount / 100)\n",
    "        elif row['selling_price'] != '':\n",
    "            return float(str(row['selling_price']).replace(',', ''))\n",
    "    return row['actual_price']\n",
    "\n",
    "df['discount'] = df['discount'].replace('', '0')\n",
    "df['actual_price'] = df.apply(impute_actual_price, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50f0e0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the remaining products without price\n",
    "df = df[(df['actual_price'] != '') & (df['selling_price'] != '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ad072e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace empty brand names with 'no brand'\n",
    "df.loc[df['brand'] == '', 'brand'] = 'no brand'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b5f0361",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(preprocess_document, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64dbd3c",
   "metadata": {},
   "source": [
    "# Deliverable 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835a6f7f",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69320f2",
   "metadata": {},
   "source": [
    "### Build index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5f4169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def normalize_cat_token(val):\n",
    "    if pd.isna(val) or str(val).strip() == \"\":\n",
    "        return []\n",
    "    # split common multi-value strings; keep a single value as 1-item list\n",
    "    parts = re.split(r\"[\\/,;|]\", str(val))\n",
    "    return [re.sub(r\"\\s+\", \"_\", p.strip().lower()) for p in parts if p.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ab1a084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['_id', 'actual_price', 'average_rating', 'brand', 'category',\n",
      "       'crawled_at', 'description', 'discount', 'images', 'out_of_stock',\n",
      "       'pid', 'product_details', 'seller', 'selling_price', 'sub_category',\n",
      "       'title', 'url'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90d15d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inverted_index_df(df: pd.DataFrame,id_col: str | None = None,text_cols: list[str] = (\"title\", \"description\", \"product_details\"),min_df: int = 1,store_positions: bool = False):\n",
    "    \"\"\"\n",
    "    df: preprocessed dataframe (title/description/product_details are token lists).\n",
    "    id_col: column holding unique ids; if None, uses df.index (as str).\n",
    "    text_cols: columns with *token lists* (already stemmed, stopwords removed).\n",
    "    min_df: drop terms that appear in < min_df documents.\n",
    "    store_positions: if True, also keep term positions for phrase/proximity queries.\n",
    "    \"\"\"\n",
    "    # assign doc ID's\n",
    "    doc_ids = df[id_col].astype(str).tolist() if id_col else df.index.astype(str).tolist()\n",
    "\n",
    "    per_doc_terms = []\n",
    "    per_doc_sequence = []\n",
    "\n",
    "    # gather tokens for each row\n",
    "    for _, row in df.iterrows():\n",
    "        tokens = []\n",
    "\n",
    "        # text cols are already tokenized lists after preprocess_document(), we just make it robust if something slipped through\n",
    "        for c in text_cols:\n",
    "            if c in df.columns:\n",
    "                vals = row[c]\n",
    "                if isinstance(vals, (list, tuple)):\n",
    "                    tokens.extend([str(t).lower() for t in vals if str(t).strip()])\n",
    "                elif pd.notna(vals):\n",
    "                    # if something slipped through as string, tokenize lightly:\n",
    "                    tokens.extend(re.findall(r\"[A-Za-z0-9]+\", str(vals).lower()))\n",
    "\n",
    "\n",
    "        # ensure we have a sequence for positions and a set for boolean presence\n",
    "        if store_positions:\n",
    "            per_doc_sequence.append(tokens[:])\n",
    "        per_doc_terms.append(set(tokens))\n",
    "\n",
    "    # build postings (term -> list[doc_id]) and df counts\n",
    "    postings_tmp = defaultdict(list)\n",
    "    df_count = defaultdict(int)\n",
    "\n",
    "    for d_i, terms in enumerate(per_doc_terms):\n",
    "        did = doc_ids[d_i]\n",
    "        for term in terms:\n",
    "            postings_tmp[term].append(did)\n",
    "            df_count[term] += 1\n",
    "\n",
    "    # min_df filter + sort postings\n",
    "    postings_tmp = {t: sorted(dids) for t, dids in postings_tmp.items() if df_count[t] >= min_df}\n",
    "\n",
    "    # vocab\n",
    "    vocab = {term: tid for tid, term in enumerate(sorted(postings_tmp.keys()))}\n",
    "    id2term = {tid: term for term, tid in vocab.items()}\n",
    "\n",
    "    # final inverted index (term_id -> [doc_ids])\n",
    "    inv_index = {vocab[t]: dids for t, dids in postings_tmp.items()}\n",
    "\n",
    "    # positional index\n",
    "    positional = None\n",
    "    if store_positions:\n",
    "        positional = {tid: defaultdict(list) for tid in inv_index.keys()}\n",
    "        for d_i, seq in enumerate(per_doc_sequence):\n",
    "            did = doc_ids[d_i]\n",
    "            for pos, tok in enumerate(seq):\n",
    "                if tok in vocab:\n",
    "                    tid = vocab[tok]\n",
    "                    positional[tid][did].append(pos)\n",
    "        # convert inner dicts to normal dicts\n",
    "        positional = {tid: dict(dmap) for tid, dmap in positional.items()}\n",
    "\n",
    "    return {\n",
    "        \"vocab\": vocab,            # term -> term_id\n",
    "        \"id2term\": id2term,        # term_id -> term\n",
    "        \"postings\": inv_index,     # term_id -> [doc_id, ...] (sorted)\n",
    "        \"doc_ids\": doc_ids,        # all doc ids, as strings\n",
    "        \"positional\": positional   # optional: term_id -> {doc_id: [positions]}\n",
    "    }\n",
    "\n",
    "index_obj = build_inverted_index_df(\n",
    "    df,\n",
    "    id_col=None,\n",
    "    text_cols=df.columns,\n",
    "    min_df=1,\n",
    "    store_positions=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ddd7f2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs for 'cotton': ['0', '1', '10', '1000', '1001', '1002', '1003', '1004', '1005', '1006', '1007', '1008', '1009', '10099', '1010', '1011', '1012', '1013', '1014', '1015', '1016', '10221', '10222', '10223', '10227', '10230', '10234', '10237', '10238', '10239', '10241', '10242', '10243', '10245', '10246', '10248', '10249', '1025', '10250', '10252', '10253', '10256', '10257', '1026', '10264', '10266', '10268', '1027', '10276', '10278']\n",
      "docs for adidas: ['250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299']\n",
      "docs for category tshirts: ['13029', '13030', '13031', '13032', '13033', '13034', '13035', '13036', '13037', '13038', '13039', '13040', '13041', '13042', '13043', '13044', '13045', '13046', '13047', '13048', '13049', '13050', '13051', '13052', '13053', '13054', '13055', '13056', '13057', '13058', '13059', '13060', '13061', '13062', '13063', '13064', '13065', '13066', '13067', '13068', '13069', '13070', '13071', '13072', '13073', '13074', '13075', '13076', '13077', '13078']\n",
      "cotton AND adidas: ['250', '251', '252', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302']\n",
      "hoodie OR sweatshirt: ['10328', '10397', '11060', '11105', '11106', '11109', '11110', '11114', '11119', '11120', '11123', '11125', '11129', '11135', '11137', '11142', '11145', '11151', '11167', '11169', '11171', '11172', '11211', '11224', '11225', '11305', '11311', '11312', '11317', '11321', '11323', '11326', '11336', '11349', '11359', '11373', '11377', '11380', '11383', '11391', '11393', '11394', '11414', '11418', '11420', '11425', '11434', '11446', '11471', '11514']\n",
      "docs for 50 ['10146', '10168', '10174', '10267', '10304', '1066', '11022', '11026', '11027', '11067', '11105', '11108', '11109', '11111', '11113', '11114', '11117', '11118', '11119', '11121', '11122', '11126', '11127', '11131', '11133', '11134', '11138', '11141', '11143', '11147', '11152', '11153', '11155', '11163', '11166', '11181', '11184', '11187', '11191', '11194', '11198', '11199', '11207', '11210', '11214', '11215', '11218', '11219', '11221', '11222']\n"
     ]
    }
   ],
   "source": [
    "# Quick lookups\n",
    "def docs_for_term(term: str):\n",
    "    \"\"\"Return document IDs for a raw term or categorical token (e.g., 'brand:nike').\"\"\"\n",
    "    tid = index_obj[\"vocab\"].get(term)\n",
    "    return index_obj[\"postings\"].get(tid, []) if tid is not None else []\n",
    "\n",
    "def doc_positions_for_term(term: str, doc_id: str):\n",
    "    \"\"\"Return positions of term in a specific document.\"\"\"\n",
    "    tid = index_obj[\"vocab\"].get(term)\n",
    "    if tid is None:\n",
    "        return []\n",
    "    return index_obj[\"positional\"].get(tid, {}).get(doc_id, [])\n",
    "\n",
    "def and_query(terms: list[str]):\n",
    "    \"\"\"Boolean AND over terms.\"\"\"\n",
    "    sets = [set(docs_for_term(t)) for t in terms]\n",
    "    return sorted(set.intersection(*sets)) if sets else []\n",
    "\n",
    "def or_query(terms: list[str]):\n",
    "    \"\"\"Boolean OR over terms.\"\"\"\n",
    "    s = set()\n",
    "    for t in terms:\n",
    "        s.update(docs_for_term(t))\n",
    "    return sorted(s)\n",
    "\n",
    "\n",
    "# Plain term from text columns \n",
    "# Print only the first results to not flood the output\n",
    "print(\"docs for 'cotton':\", docs_for_term(\"cotton\")[:50])\n",
    "print(\"docs for adidas:\", docs_for_term(\"adidas\")[:50])\n",
    "print(\"docs for category tshirts:\", docs_for_term(\"tshirts\")[:50])\n",
    "print(\"cotton AND adidas:\", and_query([\"cotton\", \"adidas\"])[:50])\n",
    "print(\"hoodie OR sweatshirt:\", or_query([\"hoodie\", \"sweatshirt\"])[:50])\n",
    "print(\"docs for 50\", docs_for_term(\"50\")[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791e90c5",
   "metadata": {},
   "source": [
    "### Example queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ae35f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tshirt', 12385), ('cotton', 9944), ('wear', 9250), ('comfort', 8654), ('shirt', 7493), ('look', 6173), ('casual', 5926), ('fit', 5600), ('made', 5591), ('fabric', 5438), ('print', 4870), ('women', 4416), ('design', 4237), ('sleev', 4009), ('qualiti', 4002), ('men', 3731), ('day', 3463), ('wash', 3453), ('style', 3241), ('make', 3233)]\n",
      "Q1 -> 30 results: ['11237', '11276', '11278', '19230', '19252', '19427', '19461', '20373', '23364', '23524']\n",
      "Q2 -> 3 results: ['254', '268', '285']\n",
      "Q3 -> 68 results: ['11502', '11513', '11527', '11580', '11596', '11628', '11629', '11637', '11648', '11657']\n",
      "Q4 -> 62 results: ['120', '122', '126', '134', '14264', '149', '17411', '23788', '26308', '26884']\n",
      "Q5 -> 56 results: ['1018', '1029', '1030', '10344', '1040', '1041', '1126', '13175', '14094', '14170']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_terms = [term for tokens in df[\"description\"] for term in tokens]\n",
    "term_freq = Counter(all_terms)\n",
    "print(term_freq.most_common(20))\n",
    "\n",
    "\n",
    "test_queries = {\n",
    "    \"Q1\": [\"cotton\", \"tshirt\", \"50\", \"100\", \"men\", \"blue\"],\n",
    "    \"Q2\": [\"adidas\", \"red\"],\n",
    "    \"Q3\": [\"denim\", \"jean\", \"skinny\"],\n",
    "    \"Q4\": [\"dress\", \"red\"],\n",
    "    \"Q5\": [\"leather\", \"jacket\"]\n",
    "}\n",
    "\n",
    "for qid, terms in test_queries.items():\n",
    "    result_docs = and_query(terms)\n",
    "    print(f\"{qid} -> {len(result_docs)} results: {result_docs[:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9f6ed5",
   "metadata": {},
   "source": [
    "### Build TF-IDF Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb0d2371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tf(word, document):\n",
    "    \"\"\"\n",
    "    Calculate term frequency for a word in a document.\n",
    "    TF = Number of times term t appears in a document\n",
    "    \"\"\"\n",
    "    return document.count(word)    \n",
    "    \n",
    "\n",
    "def calculate_idf(word, all_documents):\n",
    "    \"\"\"\n",
    "    Calculate inverse document frequency for a word.\n",
    "    IDF = log(Total number of documents / Number of documents containing term t)\n",
    "    \"\"\"\n",
    "    num_documents_with_term = len(docs_for_term(word))\n",
    "    if num_documents_with_term == 0:\n",
    "        return 0\n",
    "    return np.log(len(all_documents) / num_documents_with_term)\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors.\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
    "        return 0\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "62a2ab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents(query, documents, k):\n",
    "    \"\"\"\n",
    "    Rank documents based on TF-IDF scores for the given query.\n",
    "    Return the top k documents.\n",
    "    \"\"\"\n",
    "    all_documents = [doc[\"description\"] + doc[\"title\"] + doc[\"brand\"] for index, doc in documents.iterrows()]\n",
    "    scores = []\n",
    "\n",
    "    term_idfs = {term: calculate_idf(term, all_documents) for term in query}\n",
    "    query_vector = np.array([calculate_tf(term, query) * term_idfs[term] for term in query])\n",
    "\n",
    "    for index, doc in documents.iterrows():\n",
    "        doc_vec = []\n",
    "        doc_text = doc[\"description\"] + doc[\"title\"] + doc[\"brand\"]\n",
    "        for term in query:\n",
    "            tf = calculate_tf(term, doc_text)\n",
    "            if tf > 0:\n",
    "                # used the formula tf = 1 + log_10(count)\n",
    "                doc_vec.append((1 + np.log(tf)) * term_idfs[term])\n",
    "            else:\n",
    "                doc_vec.append(0)\n",
    "        scores.append((doc, cosine_similarity(query_vector, np.array(doc_vec))))\n",
    "\n",
    "    # Sort documents by score in descending order\n",
    "    ranked_docs = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return ranked_docs[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2eefb69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(_id                             dea0b151-727d-51d7-b902-475dc3db64c7\n",
       "  actual_price                                                   499.0\n",
       "  average_rating                                                   3.8\n",
       "  brand                                              [naaz, collectio]\n",
       "  category                                    Clothing and Accessories\n",
       "  crawled_at                                             1612999518000\n",
       "  description        [naaz, collect, bring, limit, edit, 100, cotto...\n",
       "  discount                                                          40\n",
       "  images             [https://rukminim1.flixcart.com/image/128/128/...\n",
       "  out_of_stock                                                   False\n",
       "  pid                                                 TSHFVJZ3RMKUZSJC\n",
       "  product_details    [round, neck, full, sleev, regular, cotton, je...\n",
       "  seller                                             Anishaonlinestore\n",
       "  selling_price                                                    299\n",
       "  sub_category                                                 Topwear\n",
       "  title                 [stripe, men, round, neck, grey, blue, tshirt]\n",
       "  url                https://www.flipkart.com/naaz-collections-stri...\n",
       "  Name: 16393, dtype: object,\n",
       "  np.float64(0.7221325779246235)),\n",
       " (_id                             e3392abb-7299-5a84-b786-2744f7ec077a\n",
       "  actual_price                                                  2999.0\n",
       "  average_rating                                                   3.9\n",
       "  brand                                                       [flexim]\n",
       "  category                                    Clothing and Accessories\n",
       "  crawled_at                                             1613001312000\n",
       "  description        [100, comb, cotton, 220, gsm, size, l, xl, xxl...\n",
       "  discount                                                          75\n",
       "  images             [https://rukminim1.flixcart.com/image/128/128/...\n",
       "  out_of_stock                                                   False\n",
       "  pid                                                 TSHEHZNBDW2WCFWC\n",
       "  product_details    [polo, neck, half, sleev, regular, cotton, ble...\n",
       "  seller                                                    Flexikaart\n",
       "  selling_price                                                    734\n",
       "  sub_category                                                 Topwear\n",
       "  title              [solid, men, polo, neck, dark, blue, maroon, t...\n",
       "  url                https://www.flipkart.com/fleximaa-solid-men-po...\n",
       "  Name: 19170, dtype: object,\n",
       "  np.float64(0.7221325779246235)),\n",
       " (_id                             97c4cafb-a9d6-5408-8096-54c8f0ca92fa\n",
       "  actual_price                                                  2999.0\n",
       "  average_rating                                                   3.9\n",
       "  brand                                                       [flexim]\n",
       "  category                                    Clothing and Accessories\n",
       "  crawled_at                                             1613001332000\n",
       "  description        [100, comb, cotton, 220, gsm, size, l, xl, xxl...\n",
       "  discount                                                          75\n",
       "  images             [https://rukminim1.flixcart.com/image/128/128/...\n",
       "  out_of_stock                                                   False\n",
       "  pid                                                 TSHEHZNCYSQFAUNG\n",
       "  product_details    [polo, neck, half, sleev, regular, cotton, ble...\n",
       "  seller                                                    Flexikaart\n",
       "  selling_price                                                    734\n",
       "  sub_category                                                 Topwear\n",
       "  title              [solid, men, polo, neck, dark, blue, grey, tsh...\n",
       "  url                https://www.flipkart.com/fleximaa-solid-men-po...\n",
       "  Name: 19208, dtype: object,\n",
       "  np.float64(0.7221325779246235)),\n",
       " (_id                             08a61f66-2866-53de-b6d9-7c58274544fc\n",
       "  actual_price                                                  2999.0\n",
       "  average_rating                                                   3.9\n",
       "  brand                                                       [flexim]\n",
       "  category                                    Clothing and Accessories\n",
       "  crawled_at                                             1613001338000\n",
       "  description        [100, comb, cotton, 220, gsm, size, l, xl, xxl...\n",
       "  discount                                                          75\n",
       "  images             [https://rukminim1.flixcart.com/image/128/128/...\n",
       "  out_of_stock                                                   False\n",
       "  pid                                                 TSHEHZN3TGHWQNH8\n",
       "  product_details    [polo, neck, half, sleev, regular, cotton, ble...\n",
       "  seller                                                    Flexikaart\n",
       "  selling_price                                                    734\n",
       "  sub_category                                                 Topwear\n",
       "  title              [solid, men, polo, neck, dark, blue, beig, tsh...\n",
       "  url                https://www.flipkart.com/fleximaa-solid-men-po...\n",
       "  Name: 19216, dtype: object,\n",
       "  np.float64(0.7221325779246235)),\n",
       " (_id                             3988c833-587c-5672-bbbd-833d6121fcb2\n",
       "  actual_price                                                  2999.0\n",
       "  average_rating                                                   3.9\n",
       "  brand                                                       [flexim]\n",
       "  category                                    Clothing and Accessories\n",
       "  crawled_at                                             1613001341000\n",
       "  description        [100, comb, cotton, 220, gsm, size, l, xl, xxl...\n",
       "  discount                                                          75\n",
       "  images             [https://rukminim1.flixcart.com/image/128/128/...\n",
       "  out_of_stock                                                   False\n",
       "  pid                                                 TSHEHZNDGM5AUXAD\n",
       "  product_details    [polo, neck, half, sleev, regular, cotton, ble...\n",
       "  seller                                                    Flexikaart\n",
       "  selling_price                                                    734\n",
       "  sub_category                                                 Topwear\n",
       "  title              [solid, men, polo, neck, purpl, dark, blue, ts...\n",
       "  url                https://www.flipkart.com/fleximaa-solid-men-po...\n",
       "  Name: 19223, dtype: object,\n",
       "  np.float64(0.7221325779246235))]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = rank_documents(test_queries[\"Q1\"], df, k=5)\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af48b989",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4404ad",
   "metadata": {},
   "source": [
    "### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08af6677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(retrieved_docs, relevant_docs, k):\n",
    "    \"\"\"\n",
    "    Calculate Precision@k.\n",
    "    Precision@k = (Number of relevant documents retrieved in top k) / k\n",
    "    \"\"\"\n",
    "    retrieved_at_k = retrieved_docs[:k]\n",
    "    relevant_retrieved = sum(1 for doc in retrieved_at_k if doc[\"pid\"] in relevant_docs)\n",
    "    return relevant_retrieved / k if k > 0 else 0\n",
    "\n",
    "def recall_at_k(retrieved_docs, relevant_docs, k):\n",
    "    \"\"\"\n",
    "    Calculate Recall@k.\n",
    "    Recall@k = (Number of relevant documents retrieved in top k) / (Total number of relevant documents)\n",
    "    \"\"\"\n",
    "    retrieved_at_k = retrieved_docs[:k]\n",
    "    relevant_retrieved = sum(1 for doc in retrieved_at_k if doc[\"pid\"] in relevant_docs)\n",
    "    total_relevant = len(relevant_docs)\n",
    "    return relevant_retrieved / total_relevant if total_relevant > 0 else 0\n",
    "\n",
    "def average_precision_at_k(retrieved_docs, relevant_docs, k):\n",
    "    \"\"\"\n",
    "    Calculate Average Precision@k.\n",
    "    AP@k = Average of Precision@i for each relevant document retrieved in top k\n",
    "    \"\"\"\n",
    "    retrieved_at_k = retrieved_docs[:k]\n",
    "    relevant_retrieved = 0\n",
    "    precision_sum = 0\n",
    "\n",
    "    for i, doc in enumerate(retrieved_at_k, start=1):\n",
    "        if doc[\"pid\"] in relevant_docs:\n",
    "            relevant_retrieved += 1\n",
    "            precision_sum += relevant_retrieved / i\n",
    "\n",
    "    return precision_sum / relevant_retrieved if relevant_retrieved > 0 else 0\n",
    "\n",
    "def f1_score(precision, recall):\n",
    "    \"\"\"\n",
    "    Calculate F1 Score.\n",
    "    F1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "    \"\"\"\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def f1_score_at_k(retrieved_docs, relevant_docs, k):\n",
    "    \"\"\"\n",
    "    Calculate F1 Score at k.\n",
    "    \"\"\"\n",
    "    precision = precision_at_k(retrieved_docs, relevant_docs, k)\n",
    "    recall = recall_at_k(retrieved_docs, relevant_docs, k)\n",
    "    return f1_score(precision, recall)\n",
    "\n",
    "def mean_average_precision(retrieved_docs_list, relevant_docs_list, k):\n",
    "    \"\"\"\n",
    "    Calculate Mean Average Precision (MAP) at k.\n",
    "    MAP = Mean of Average Precision@k over all queries\n",
    "    \"\"\"\n",
    "    ap_sum = 0\n",
    "    num_queries = len(retrieved_docs_list)\n",
    "\n",
    "    for retrieved_docs, relevant_docs in zip(retrieved_docs_list, relevant_docs_list):\n",
    "        ap_sum += average_precision_at_k(retrieved_docs, relevant_docs, k)\n",
    "\n",
    "    return ap_sum / num_queries if num_queries > 0 else 0\n",
    "\n",
    "def reciprocal_rank(retrieved_docs, relevant_docs):\n",
    "    \"\"\"\n",
    "    Calculate Reciprocal Rank (RR).\n",
    "    RR = 1 / Rank of the first relevant document\n",
    "    \"\"\"\n",
    "    rank = 0\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        if doc[\"pid\"] in relevant_docs:\n",
    "            rank = i + 1\n",
    "            break\n",
    "    return 1 / rank if rank > 0 else 0\n",
    "\n",
    "def mean_reciprocal_rank(retrieved_docs_list, relevant_docs_list):\n",
    "    \"\"\"\n",
    "    Calculate Mean Reciprocal Rank (MRR).\n",
    "    MRR = Mean of Reciprocal Ranks over all queries\n",
    "    \"\"\"\n",
    "    rr_sum = 0\n",
    "    num_queries = len(retrieved_docs_list)\n",
    "\n",
    "    for retrieved_docs, relevant_docs in zip(retrieved_docs_list, relevant_docs_list):\n",
    "        rr_sum += reciprocal_rank(retrieved_docs, relevant_docs)\n",
    "\n",
    "    return rr_sum / num_queries if num_queries > 0 else 0\n",
    "\n",
    "def dcg_at_k(retrieved_docs, relevant_docs, k):\n",
    "    \"\"\"\n",
    "    Calculate Discounted Cumulative Gain (DCG) at k.\n",
    "    DCG@k = Sum of (relevance of document at rank i) / log2(i + 1) for i in 1 to k\n",
    "    \"\"\"\n",
    "    dcg = 0\n",
    "    for i in range(min(k, len(retrieved_docs))):\n",
    "        doc = retrieved_docs[i]\n",
    "        if doc[\"pid\"] in relevant_docs:\n",
    "            relevance = 1  # we only have binary relevance\n",
    "        else:\n",
    "            relevance = 0\n",
    "        dcg += relevance / np.log2(i + 2)  # i + 2 because i starts from 0\n",
    "    return dcg\n",
    "\n",
    "def ndcg_at_k(retrieved_docs, relevant_docs, k):\n",
    "    \"\"\"\n",
    "    Calculate Normalized Discounted Cumulative Gain (NDCG) at k.\n",
    "    NDCG@k = DCG@k / IDCG@k\n",
    "    \"\"\"\n",
    "    dcg = dcg_at_k(retrieved_docs, relevant_docs, k)\n",
    "    \n",
    "    ideal_retrieved_docs = [{\"pid\": pid} for pid in relevant_docs]\n",
    "    idcg = dcg_at_k(ideal_retrieved_docs, relevant_docs, k)\n",
    "    \n",
    "    return dcg / idcg if idcg > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f69451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THIS PATH AS NEEDED\n",
    "relevant_docs_path = \"../../data/validation_labels.csv\"\n",
    "relevant_df = pd.read_csv(relevant_docs_path)\n",
    "relevant_q1 = relevant_df[relevant_df['query_id'] == 1]['pid'].tolist()\n",
    "relevant_q2 = relevant_df[relevant_df['query_id'] == 2]['pid'].tolist()\n",
    "\n",
    "example_q1 = \"women full sleeve sweatshirt cotton\".split()\n",
    "example_q2 = \"men slim jeans blue\".split()\n",
    "\n",
    "out1 = rank_documents(example_q1, df, k=15)\n",
    "out2 = rank_documents(example_q2, df, k=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b0ca5d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(_id                             b7d17523-6ce4-51fe-b56f-5c61e3c122c7\n",
       "  actual_price                                                  2599.0\n",
       "  average_rating                                                   4.0\n",
       "  brand                                                         [voxa]\n",
       "  category                                    Clothing and Accessories\n",
       "  crawled_at                                             1612990563000\n",
       "  description        [cotton, fleec, blend, slim, fit, two, front, ...\n",
       "  discount                                                          76\n",
       "  images             [https://rukminim1.flixcart.com/image/128/128/...\n",
       "  out_of_stock                                                   False\n",
       "  pid                                                 SWSFN8YZ33MHKQVF\n",
       "  product_details    [maroon, cotton, fleec, blend, solid, rib, col...\n",
       "  seller                                               LAFANTAR E-TAIL\n",
       "  selling_price                                                    598\n",
       "  sub_category                                             Winter Wear\n",
       "  title                        [full, sleev, solid, women, sweatshirt]\n",
       "  url                https://www.flipkart.com/voxati-full-sleeve-so...\n",
       "  Name: 4047, dtype: object,\n",
       "  np.float64(0.7858599750208979)),\n",
       " (_id                             40ec6442-5cda-586b-9747-11cca10ca6b6\n",
       "  actual_price                                                  2599.0\n",
       "  average_rating                                                   4.0\n",
       "  brand                                                         [voxa]\n",
       "  category                                    Clothing and Accessories\n",
       "  crawled_at                                             1612990571000\n",
       "  description        [cotton, fleec, blend, slim, fit, two, front, ...\n",
       "  discount                                                          76\n",
       "  images             [https://rukminim1.flixcart.com/image/128/128/...\n",
       "  out_of_stock                                                   False\n",
       "  pid                                                 SWSFN8YZXFSUCAJX\n",
       "  product_details    [blue, cotton, fleec, blend, solid, rib, colla...\n",
       "  seller                                               LAFANTAR E-TAIL\n",
       "  selling_price                                                    598\n",
       "  sub_category                                             Winter Wear\n",
       "  title                        [full, sleev, solid, women, sweatshirt]\n",
       "  url                https://www.flipkart.com/voxati-full-sleeve-so...\n",
       "  Name: 4058, dtype: object,\n",
       "  np.float64(0.7858599750208979)),\n",
       " (_id                             aa8c3de3-ed17-5ac0-b369-4b7776658c83\n",
       "  actual_price                                                  2599.0\n",
       "  average_rating                                                   4.0\n",
       "  brand                                                         [voxa]\n",
       "  category                                    Clothing and Accessories\n",
       "  crawled_at                                             1612990575000\n",
       "  description        [cotton, fleec, blend, slim, fit, two, front, ...\n",
       "  discount                                                          76\n",
       "  images             [https://rukminim1.flixcart.com/image/128/128/...\n",
       "  out_of_stock                                                   False\n",
       "  pid                                                 SWSFN8YZZAN8RBDC\n",
       "  product_details    [black, cotton, fleec, blend, solid, rib, coll...\n",
       "  seller                                               LAFANTAR E-TAIL\n",
       "  selling_price                                                    598\n",
       "  sub_category                                             Winter Wear\n",
       "  title                        [full, sleev, solid, women, sweatshirt]\n",
       "  url                https://www.flipkart.com/voxati-full-sleeve-so...\n",
       "  Name: 4063, dtype: object,\n",
       "  np.float64(0.7858599750208979)),\n",
       " (_id                             9a5c900d-7b57-5e75-9382-3f36b8cc9ede\n",
       "  actual_price                                                  2599.0\n",
       "  average_rating                                                   4.0\n",
       "  brand                                                         [voxa]\n",
       "  category                                    Clothing and Accessories\n",
       "  crawled_at                                             1612990582000\n",
       "  description        [cotton, fleec, blend, slim, fit, two, front, ...\n",
       "  discount                                                          76\n",
       "  images             [https://rukminim1.flixcart.com/image/128/128/...\n",
       "  out_of_stock                                                   False\n",
       "  pid                                                 SWSFN8YZW4VPXW7W\n",
       "  product_details    [grey, cotton, fleec, blend, solid, rib, colla...\n",
       "  seller                                               LAFANTAR E-TAIL\n",
       "  selling_price                                                    598\n",
       "  sub_category                                             Winter Wear\n",
       "  title                        [full, sleev, solid, women, sweatshirt]\n",
       "  url                https://www.flipkart.com/voxati-full-sleeve-so...\n",
       "  Name: 4070, dtype: object,\n",
       "  np.float64(0.7858599750208979)),\n",
       " (_id                             7481eb8c-d00b-5a16-ae55-f8f2d4897570\n",
       "  actual_price                                                  1549.0\n",
       "  average_rating                                                   2.5\n",
       "  brand                                                    [ecko, unl]\n",
       "  category                                    Clothing and Accessories\n",
       "  crawled_at                                             1612996360000\n",
       "  description        [ecko, unltd, solid, 70, cotton, 30, poly, sli...\n",
       "  discount                                                          18\n",
       "  images             [https://rukminim1.flixcart.com/image/128/128/...\n",
       "  out_of_stock                                                   False\n",
       "  pid                                                 SWSFV5JN5TKJWPZ2\n",
       "  product_details    [green, cotton, blend, solid, hood, neck, full...\n",
       "  seller                                                SandSMarketing\n",
       "  selling_price                                                   1270\n",
       "  sub_category                                             Winter Wear\n",
       "  title                        [full, sleev, solid, women, sweatshirt]\n",
       "  url                https://www.flipkart.com/ecko-unltd-full-sleev...\n",
       "  Name: 11850, dtype: object,\n",
       "  np.float64(0.7858599750208979))]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(_id                             db53a2e8-3b7d-59fb-9aa9-f8c1b4492e7b\n",
       "  actual_price                                                  3299.0\n",
       "  average_rating                                                   4.3\n",
       "  brand                                                    [no, brand]\n",
       "  category                                    Clothing and Accessories\n",
       "  crawled_at                                             1612987934000\n",
       "  description        [wear, eleg, man, tie, effect, way, make, look...\n",
       "  discount                                                          72\n",
       "  images             [https://rukminim1.flixcart.com/image/128/128/...\n",
       "  out_of_stock                                                   False\n",
       "  pid                                                 CTPFVZT7EFZWVRUP\n",
       "  product_details    [formalcasu, print, polyest, neckti, set, pock...\n",
       "  seller                                               NextEdgeRetails\n",
       "  selling_price                                                    899\n",
       "  sub_category                                    Clothing Accessories\n",
       "  title                            [nulit, satin, tie, cufflink, blue]\n",
       "  url                https://www.flipkart.com/nu-lite-satin-tie-cuf...\n",
       "  Name: 45, dtype: object,\n",
       "  np.float64(0.5102122940135733)),\n",
       " (_id                             6d35409c-7412-5556-bc13-f0077e8ba7b5\n",
       "  actual_price                                                  3299.0\n",
       "  average_rating                                                   2.7\n",
       "  brand                                                    [no, brand]\n",
       "  category                                    Clothing and Accessories\n",
       "  crawled_at                                             1612987952000\n",
       "  description        [wear, eleg, man, tie, effect, way, make, look...\n",
       "  discount                                                          81\n",
       "  images             [https://rukminim1.flixcart.com/image/128/128/...\n",
       "  out_of_stock                                                   False\n",
       "  pid                                                 CTPFVZTBCHWHDMGJ\n",
       "  product_details    [formalcasu, print, polyest, neckti, set, pock...\n",
       "  seller                                               NextEdgeRetails\n",
       "  selling_price                                                    599\n",
       "  sub_category                                    Clothing Accessories\n",
       "  title                            [nulit, satin, tie, cufflink, blue]\n",
       "  url                https://www.flipkart.com/nu-lite-satin-tie-cuf...\n",
       "  Name: 89, dtype: object,\n",
       "  np.float64(0.5102122940135733)),\n",
       " (_id                             8d92559c-6538-5f0c-bc6a-a880207b789d\n",
       "  actual_price                                                  3299.0\n",
       "  average_rating                                                   2.7\n",
       "  brand                                                    [no, brand]\n",
       "  category                                    Clothing and Accessories\n",
       "  crawled_at                                             1612987960000\n",
       "  description        [wear, eleg, man, tie, effect, way, make, look...\n",
       "  discount                                                          81\n",
       "  images             [https://rukminim1.flixcart.com/image/128/128/...\n",
       "  out_of_stock                                                   False\n",
       "  pid                                                 CTPFVZT6GAFZ7BJX\n",
       "  product_details    [formalcasu, print, polyest, neckti, set, pock...\n",
       "  seller                                               NextEdgeRetails\n",
       "  selling_price                                                    599\n",
       "  sub_category                                    Clothing Accessories\n",
       "  title                            [nulit, satin, tie, cufflink, blue]\n",
       "  url                https://www.flipkart.com/nu-lite-satin-tie-cuf...\n",
       "  Name: 106, dtype: object,\n",
       "  np.float64(0.5102122940135733)),\n",
       " (_id                             b60846e6-8e03-55fb-8cd1-a9f567e6461e\n",
       "  actual_price                                                  1599.0\n",
       "  average_rating                                                   3.3\n",
       "  brand                                                [ifg, collecti]\n",
       "  category                                    Clothing and Accessories\n",
       "  crawled_at                                             1612990073000\n",
       "  description                                                       []\n",
       "  discount                                                          68\n",
       "  images             [https://rukminim1.flixcart.com/image/128/128/...\n",
       "  out_of_stock                                                   False\n",
       "  pid                                                 JEAFYGU2GXZZJTZF\n",
       "  product_details    [ifgcjdb28, men, western, wear, 1, curv, pocke...\n",
       "  seller                                                   Silverspear\n",
       "  selling_price                                                    499\n",
       "  sub_category                                              Bottomwear\n",
       "  title                                  [slim, men, dark, blue, jean]\n",
       "  url                https://www.flipkart.com/ifg-collection-slim-m...\n",
       "  Name: 3423, dtype: object,\n",
       "  np.float64(0.5102122940135733)),\n",
       " (_id                             f6c787de-9d18-55ad-a60e-d70eed066099\n",
       "  actual_price                                                  2499.0\n",
       "  average_rating                                                   3.0\n",
       "  brand                                                          [wab]\n",
       "  category                                    Clothing and Accessories\n",
       "  crawled_at                                             1612991795000\n",
       "  description        [sit, waist, slimfit, hip, thigh, slimleg, mad...\n",
       "  discount                                                          28\n",
       "  images             [https://rukminim1.flixcart.com/image/128/128/...\n",
       "  out_of_stock                                                   False\n",
       "  pid                                                 JEAERYGSGHGM9JBA\n",
       "  product_details    [wf0242, men, western, wear, 1, coin, pocket, ...\n",
       "  seller                                                      WabbaJea\n",
       "  selling_price                                                   1799\n",
       "  sub_category                                              Bottomwear\n",
       "  title                                        [slim, men, blue, jean]\n",
       "  url                https://www.flipkart.com/wabba-slim-men-blue-j...\n",
       "  Name: 5776, dtype: object,\n",
       "  np.float64(0.5102122940135733))]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show only the first 5 results for brevity\n",
    "display(out1[:5])\n",
    "display(out2[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19e11473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation for Query 1:\n",
      "  k=15: Precision@k=0.067, Recall@k=0.050, F1@k=0.057, AP@k=0.500, NDCG@k=0.108\n",
      "\n",
      "Evaluation for Query 2:\n",
      "  k=15: Precision@k=0.067, Recall@k=0.050, F1@k=0.057, AP@k=0.083, NDCG@k=0.046\n",
      "\n",
      "MAP@15: 0.292, MRR = 0.292\n"
     ]
    }
   ],
   "source": [
    "# remove score as it is not needed for evaluation functions\n",
    "retrieved1 = [doc for doc, score in out1]\n",
    "retrieved2 = [doc for doc, score in out2]\n",
    "\n",
    "def show_top_k(out, k=5):\n",
    "    return [(doc['pid'], float(score)) for doc, score in out[:k]]\n",
    "\n",
    "\n",
    "# ks to evaluate, we put only 15 to not clutter the output\n",
    "ks = [15]\n",
    "\n",
    "print('\\nEvaluation for Query 1:')\n",
    "for k in ks:\n",
    "    p = precision_at_k(retrieved1, relevant_q1, k)\n",
    "    r = recall_at_k(retrieved1, relevant_q1, k)\n",
    "    f1 = f1_score_at_k(retrieved1, relevant_q1, k)\n",
    "    ap = average_precision_at_k(retrieved1, relevant_q1, k)\n",
    "    ndcg = ndcg_at_k(retrieved1, relevant_q1, k)\n",
    "    print(f'  k={k}: Precision@k={p:.3f}, Recall@k={r:.3f}, F1@k={f1:.3f}, AP@k={ap:.3f}, NDCG@k={ndcg:.3f}')\n",
    "\n",
    "print('\\nEvaluation for Query 2:')\n",
    "for k in ks:\n",
    "    p = precision_at_k(retrieved2, relevant_q2, k)\n",
    "    r = recall_at_k(retrieved2, relevant_q2, k)\n",
    "    f1 = f1_score_at_k(retrieved2, relevant_q2, k)\n",
    "    ap = average_precision_at_k(retrieved2, relevant_q2, k)\n",
    "    ndcg = ndcg_at_k(retrieved2, relevant_q2, k)\n",
    "    print(f'  k={k}: Precision@k={p:.3f}, Recall@k={r:.3f}, F1@k={f1:.3f}, AP@k={ap:.3f}, NDCG@k={ndcg:.3f}')\n",
    "\n",
    "# MAP@k and MRR across the two queries\n",
    "for k in [15]:\n",
    "    map_k = mean_average_precision([retrieved1, retrieved2], [relevant_q1, relevant_q2], k)\n",
    "    mrr_k = mean_reciprocal_rank([retrieved1, retrieved2], [relevant_q1, relevant_q2])\n",
    "    print(f'\\nMAP@{k}: {map_k:.3f}, MRR = {mrr_k:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877bcc64",
   "metadata": {},
   "source": [
    "### Proposed queries evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71d7dffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation for query: cotton tshirt 50 100 men blue\n",
      "  k=5: Precision@k=0.000, Recall@k=0.000, F1@k=0.000, AP@k=0.000, NDCG@k=0.000\n",
      "  k=10: Precision@k=0.000, Recall@k=0.000, F1@k=0.000, AP@k=0.000, NDCG@k=0.000\n",
      "\n",
      "Evaluation for query: adidas red\n",
      "  k=5: Precision@k=0.600, Recall@k=1.000, F1@k=0.750, AP@k=1.000, NDCG@k=1.000\n",
      "  k=10: Precision@k=0.300, Recall@k=1.000, F1@k=0.462, AP@k=1.000, NDCG@k=1.000\n",
      "\n",
      "Evaluation for query: denim jean skinny\n",
      "  k=5: Precision@k=0.000, Recall@k=0.000, F1@k=0.000, AP@k=0.000, NDCG@k=0.000\n",
      "  k=10: Precision@k=0.000, Recall@k=0.000, F1@k=0.000, AP@k=0.000, NDCG@k=0.000\n",
      "\n",
      "Evaluation for query: dress red\n",
      "  k=5: Precision@k=0.000, Recall@k=0.000, F1@k=0.000, AP@k=0.000, NDCG@k=0.000\n",
      "  k=10: Precision@k=0.300, Recall@k=0.083, F1@k=0.130, AP@k=0.216, NDCG@k=0.199\n",
      "\n",
      "Evaluation for query: leather jacket\n",
      "  k=5: Precision@k=1.000, Recall@k=0.143, F1@k=0.250, AP@k=1.000, NDCG@k=1.000\n",
      "  k=10: Precision@k=1.000, Recall@k=0.286, F1@k=0.444, AP@k=1.000, NDCG@k=1.000\n",
      "\n",
      "MAP@5: 0.400, MRR = 0.425\n",
      "\n",
      "MAP@10: 0.443, MRR = 0.425\n"
     ]
    }
   ],
   "source": [
    "def get_pid_from_id(doc_id: str):\n",
    "    return df.iloc[int(doc_id)]['pid']\n",
    "\n",
    "# choice to automatically assign relevance (instead of going through some documents one by one):\n",
    "# -retrieve all documents which contain all words in the query (like previous task)\n",
    "# -if the number of retrieved documents is <MIN_AMOUNT, keep them all\n",
    "# -otherwise keep a small percentage of the remaining documents after MIN_AMOUNT\n",
    "\n",
    "MIN_AMOUNT = 30\n",
    "PERCENTAGE = 0.20\n",
    "relevant_docs = []\n",
    "\n",
    "for qid, terms in test_queries.items():\n",
    "    result_docs = and_query(terms)\n",
    "    docs_to_retrieve = len(result_docs) if len(result_docs) < MIN_AMOUNT else MIN_AMOUNT + int(PERCENTAGE * (len(result_docs) - MIN_AMOUNT))\n",
    "    result_docs = [get_pid_from_id(doc_id) for doc_id in result_docs[:docs_to_retrieve]]\n",
    "    relevant_docs.append(result_docs)\n",
    "\n",
    "retrieved_docs_list = [rank_documents(query, df, k=10) for query in test_queries.values()]\n",
    "# remove the score from the retrieved docs as it is not needed for evaluation\n",
    "retrieved_docs_list = [[doc for doc, score in out] for out in retrieved_docs_list]\n",
    "ks = [5, 10]\n",
    "\n",
    "for i, query in enumerate(test_queries.values()):\n",
    "    retrieved = retrieved_docs_list[i]\n",
    "    print('\\nEvaluation for query: ' + ' '.join(query))\n",
    "    for k in ks:\n",
    "        p = precision_at_k(retrieved, relevant_docs[i], k)\n",
    "        r = recall_at_k(retrieved, relevant_docs[i], k)\n",
    "        f1 = f1_score_at_k(retrieved, relevant_docs[i], k)\n",
    "        ap = average_precision_at_k(retrieved, relevant_docs[i], k)\n",
    "        ndcg = ndcg_at_k(retrieved, relevant_docs[i], k)\n",
    "        print(f'  k={k}: Precision@k={p:.3f}, Recall@k={r:.3f}, F1@k={f1:.3f}, AP@k={ap:.3f}, NDCG@k={ndcg:.3f}')\n",
    "\n",
    "for k in ks:\n",
    "    map_k = mean_average_precision(retrieved_docs_list, relevant_docs, k)\n",
    "    mrr_k = mean_reciprocal_rank(retrieved_docs_list, relevant_docs)\n",
    "    print(f'\\nMAP@{k}: {map_k:.3f}, MRR = {mrr_k:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f8f4ca",
   "metadata": {},
   "source": [
    "## Evaluation metrics explanation and analysis\n",
    "\n",
    "### What each metric measures\n",
    "\n",
    "- **Precision@k**: fraction of the top‑k results that are relevant. Uses only the top‑k set and binary relevance judgments.\n",
    "\n",
    "- **Recall@k**: fraction of all relevant documents that appear in the top‑k. Uses top‑k and the full set of known relevant docs.\n",
    "\n",
    "- **F1@k**: harmonic mean of Precision@k and Recall@k. Summarizes the balance between precision and recall at a given k.\n",
    "\n",
    "- **Average Precision (AP@k)**: mean of precision values computed at ranks where relevant documents occur (truncated at k). Uses ordering information within top‑k and rewards early placement of relevant docs.\n",
    "\n",
    "- **Mean Average Precision (MAP@k)**: mean of AP@k across queries. Used to assess the quality of the retrieval system.\n",
    "\n",
    "- **Reciprocal Rank (RR) and Mean Reciprocal Rank (MRR)**: RR = 1 / rank of first relevant doc; MRR averages RR across queries. Uses only the first relevant hit.\n",
    "\n",
    "- **DCG@k / NDCG@k**: discounted cumulative gain accounts for position (log discount); NDCG normalizes by ideal DCG so scores are comparable. Generally used with graded relevance (with binary relevance it simply accounts for discounted hits).\n",
    "\n",
    "### Result interpretation\n",
    "\n",
    "- High precision, low recall (small k): top results are clean but many relevant docs fall outside top‑k. To fix this we can increase recall via broader matching or query expansion, raise k, or tune retrieval. (This is the case for Q5)\n",
    "\n",
    "- Low precision, high recall: system retrieves many relevant items but top positions contain non many non relevant documents. To fix this improve ranking, incorporate quality/popularity signals. May also be the fact there are not many relevant documents to begin with (like Q2, where one should actually use k = min(chosen_k, len(relevant_documents)))\n",
    "\n",
    "- All metrics low: it can mean multiple things: preprocessing is not done well, tokenization may have mismatch, there could be indexing problems, or simply the retrieval model is too weak. (This is the problem for the other 3 queries, where the queries are pretty general and contain frequent terms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19fa481",
   "metadata": {},
   "source": [
    "### Retrieval problems and solution proposals\n",
    "\n",
    "Right now the index and ranking focus almost entirely on token overlap in the text fields, but a lot of useful information in the product records is effectively ignored. Numeric fields like price, discount and rating are converted/cleaned but not used as ranked signals even though they can be powerful relevance cues for users (cheap vs premium, high rating, big discount), yet the current pipeline treats them as side data rather than features that should influence ranking. That means two items that match the query text equally well may be ordered arbitrarily even though one is clearly preferable by price or rating.\n",
    "\n",
    "Another important issue is that all text fields are treated the same. Title, brand and description are concatenated or given equal treatment in scoring, so a match in a brand string carries the same weight as the same match in a long description. In practice users care much more about matches in short, high‑precision fields (title, brand) than in noisy long fields. Not giving fields different weights dilutes strong signals and reduces ranking accuracy.\n",
    "\n",
    "Finally, the ranking ignores word order and phrases. The system is effectively bag‑of‑words: it counts tokens but doesn’t reward exact phrases or tokens that occur close together. Queries like “slim blue jeans” or “full sleeve sweatshirt” lose meaning when order and proximity aren’t considered; documents that contain the words scattered across different parts of the page can be ranked equally to documents that contain the exact phrase, which harms perceived relevance.\n",
    "\n",
    "In short: useful numeric signals aren’t used as ranking features, field importance is flattened, and phrase/proximity information is lost. Some fixes proporals are the followings: treat numeric fields as features or filters, boost short/high‑precision fields (title, brand) when scoring, and enable positional/phrase handling (N-grams instead of unigrams)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
