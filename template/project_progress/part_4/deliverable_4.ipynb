{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38d2ad7a",
   "metadata": {},
   "source": [
    "# Deliverable 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2fbece",
   "metadata": {},
   "source": [
    "SNumbers: u264332, u264443, u264202\n",
    "\n",
    "Names: Levente Olivér Bódi, Riccardo Zamuner, Giada Izzo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b3b603",
   "metadata": {},
   "source": [
    "## Previous deliverable code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd734c6",
   "metadata": {},
   "source": [
    "This is the same code of the previous deliverable minus prints and commentary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17d228d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import uuid\n",
    "import datetime\n",
    "from flask import Flask, request, render_template_string, redirect, url_for, jsonify\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65b9f84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess a text by tokenizing, lowercasing, removing stop words, and stemming.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # Remove punctuation\n",
    "    tokens = [re.sub(r\"[^\\w\\s]\", \"\", token) for token in tokens]\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Remove punctuation\n",
    "    tokens = [re.sub(r\"[^\\w\\s]\", \"\", token) for token in tokens]\n",
    "\n",
    "    # Stem the tokens\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    # Remove empty strings\n",
    "    tokens = [token for token in tokens if token]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def clean_seller(text):\n",
    "    \"\"\"\n",
    "    Clean the seller field by removing unwanted trailing phrases.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove unwanted trailing phrases and everything after them\n",
    "    remove_phrases = [\n",
    "        \"Seller changed\",\n",
    "        \"(Not Enough Ratin\",\n",
    "        \"(New Sell\"\n",
    "    ]\n",
    "    for phrase in remove_phrases:\n",
    "        idx = text.find(phrase)\n",
    "        if idx != -1:\n",
    "            text = text[:idx]\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_non_textual(document):\n",
    "    \"\"\"\n",
    "    Preprocess non-textual fields in the document.\n",
    "    \"\"\"\n",
    "\n",
    "    # Discount preprocessing: convert from string \"xx% off\" to integer xx\n",
    "    # also taking into account documents without discount\n",
    "    if isinstance(document[\"discount\"], str) and \"%\" in document[\"discount\"]:\n",
    "        document[\"discount\"] = int(document[\"discount\"][:document[\"discount\"].find(\"%\")])\n",
    "    else:\n",
    "        document[\"discount\"] = 0\n",
    "        \n",
    "    # Merge all values from product_details dictionary and preprocess\n",
    "    if isinstance(document[\"product_details\"], dict):\n",
    "        details_text = \" \".join(str(v) for v in document[\"product_details\"].values())\n",
    "    elif isinstance(document[\"product_details\"], list):\n",
    "        # If it's a list of dicts, merge all values from all dicts\n",
    "        details_text = \" \".join(str(v) for d in document[\"product_details\"] if isinstance(d, dict) for v in d.values())\n",
    "    else:\n",
    "        details_text = str(document[\"product_details\"])\n",
    "    document[\"product_details\"] = preprocess_text(details_text)\n",
    "\n",
    "    # Convert actual_price and selling_price to integers (remove commas)\n",
    "    # If actual_price is NaN, set it to discounted selling_price\n",
    "    for price_field in [\"actual_price\", \"selling_price\"]:\n",
    "        if isinstance(document[price_field], str):\n",
    "            price_str = document[price_field].replace(\",\", \"\")\n",
    "            price_val = price_str.split(\".\")[0]\n",
    "            document[price_field] = int(price_val) if price_val.isdigit() else 0\n",
    "\n",
    "    # If actual_price is missing or zero, set it to discounted selling_price\n",
    "    if (\"actual_price\" not in document or document[\"actual_price\"] == 0) and \"selling_price\" in document:\n",
    "        document[\"actual_price\"] = int(int(document[\"selling_price\"])*document[\"discount\"]/100)\n",
    "\n",
    "    # Convert average_rating to float, set to NaN if missing or empty\n",
    "    if \"average_rating\" in document and str(document[\"average_rating\"]).strip() != \"\":\n",
    "        try:\n",
    "            document[\"average_rating\"] = float(document[\"average_rating\"])\n",
    "        except ValueError:\n",
    "            document[\"average_rating\"] = float(\"nan\")\n",
    "    else:\n",
    "        document[\"average_rating\"] = float(\"nan\")\n",
    "\n",
    "    return document\n",
    "\n",
    "def preprocess_document(document):\n",
    "    \"\"\"\n",
    "    Join all preprocessing steps for a document.\n",
    "    \"\"\"\n",
    "\n",
    "    document[\"description\"] = preprocess_text(document[\"description\"])\n",
    "    document[\"title\"] = preprocess_text(document[\"title\"])\n",
    "    document[\"seller\"] = clean_seller(document[\"seller\"])\n",
    "    document[\"brand\"] = document[\"brand\"].lower().split()\n",
    "\n",
    "    document = preprocess_non_textual(document)\n",
    "\n",
    "    return document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "977d4751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFY THIS PATH AS NEEDED\n",
    "file_path = \"../../data/fashion_products_dataset.json\"\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff0e1f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_actual_price(row):\n",
    "    # if actual_price is empty, try to compute it:\n",
    "    # either from selling_price and discount, or just use selling_price\n",
    "    if row['actual_price'] == '':\n",
    "        # Convert selling_price and discount to float if not empty\n",
    "        if row['selling_price'] != '' and row['discount'] != '':\n",
    "            selling_price = float(str(row['selling_price']).replace(',', ''))\n",
    "            discount = float(str(row['discount']).replace('%', '').replace('off', '').strip())\n",
    "            return selling_price * (1 - discount / 100)\n",
    "        elif row['selling_price'] != '':\n",
    "            return float(str(row['selling_price']).replace(',', ''))\n",
    "    return row['actual_price']\n",
    "\n",
    "df['discount'] = df['discount'].replace('', '0')\n",
    "df['actual_price'] = df.apply(impute_actual_price, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50f0e0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the remaining products without price\n",
    "df = df[(df['actual_price'] != '') & (df['selling_price'] != '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ad072e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace empty brand names with 'no brand'\n",
    "df.loc[df['brand'] == '', 'brand'] = 'no brand'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b5f0361",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(preprocess_document, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5f4169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def normalize_cat_token(val):\n",
    "    if pd.isna(val) or str(val).strip() == \"\":\n",
    "        return []\n",
    "    # split common multi-value strings; keep a single value as 1-item list\n",
    "    parts = re.split(r\"[\\/,;|]\", str(val))\n",
    "    return [re.sub(r\"\\s+\", \"_\", p.strip().lower()) for p in parts if p.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90d15d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inverted_index_df(df: pd.DataFrame,id_col: str | None = None,text_cols: list[str] = (\"title\", \"description\", \"product_details\"),min_df: int = 1,store_positions: bool = False):\n",
    "    \"\"\"\n",
    "    df: preprocessed dataframe (title/description/product_details are token lists).\n",
    "    id_col: column holding unique ids; if None, uses df.index (as str).\n",
    "    text_cols: columns with *token lists* (already stemmed, stopwords removed).\n",
    "    min_df: drop terms that appear in < min_df documents.\n",
    "    store_positions: if True, also keep term positions for phrase/proximity queries.\n",
    "    \"\"\"\n",
    "    # assign doc ID's\n",
    "    doc_ids = df[id_col].astype(str).tolist() if id_col else df.index.astype(str).tolist()\n",
    "\n",
    "    per_doc_terms = []\n",
    "    per_doc_sequence = []\n",
    "\n",
    "    # gather tokens for each row\n",
    "    for _, row in df.iterrows():\n",
    "        tokens = []\n",
    "\n",
    "        # text cols are already tokenized lists after preprocess_document(), we just make it robust if something slipped through\n",
    "        for c in text_cols:\n",
    "            if c in df.columns:\n",
    "                vals = row[c]\n",
    "                if isinstance(vals, (list, tuple)):\n",
    "                    tokens.extend([str(t).lower() for t in vals if str(t).strip()])\n",
    "                elif pd.notna(vals):\n",
    "                    # if something slipped through as string, tokenize lightly:\n",
    "                    tokens.extend(re.findall(r\"[A-Za-z0-9]+\", str(vals).lower()))\n",
    "\n",
    "\n",
    "        # ensure we have a sequence for positions and a set for boolean presence\n",
    "        if store_positions:\n",
    "            per_doc_sequence.append(tokens[:])\n",
    "        per_doc_terms.append(set(tokens))\n",
    "\n",
    "    # build postings (term -> list[doc_id]) and df counts\n",
    "    postings_tmp = defaultdict(list)\n",
    "    df_count = defaultdict(int)\n",
    "\n",
    "    for d_i, terms in enumerate(per_doc_terms):\n",
    "        did = doc_ids[d_i]\n",
    "        for term in terms:\n",
    "            postings_tmp[term].append(did)\n",
    "            df_count[term] += 1\n",
    "\n",
    "    # min_df filter + sort postings\n",
    "    postings_tmp = {t: sorted(dids) for t, dids in postings_tmp.items() if df_count[t] >= min_df}\n",
    "\n",
    "    # vocab\n",
    "    vocab = {term: tid for tid, term in enumerate(sorted(postings_tmp.keys()))}\n",
    "    id2term = {tid: term for term, tid in vocab.items()}\n",
    "\n",
    "    # final inverted index (term_id -> [doc_ids])\n",
    "    inv_index = {vocab[t]: dids for t, dids in postings_tmp.items()}\n",
    "\n",
    "    # positional index\n",
    "    positional = None\n",
    "    if store_positions:\n",
    "        positional = {tid: defaultdict(list) for tid in inv_index.keys()}\n",
    "        for d_i, seq in enumerate(per_doc_sequence):\n",
    "            did = doc_ids[d_i]\n",
    "            for pos, tok in enumerate(seq):\n",
    "                if tok in vocab:\n",
    "                    tid = vocab[tok]\n",
    "                    positional[tid][did].append(pos)\n",
    "        # convert inner dicts to normal dicts\n",
    "        positional = {tid: dict(dmap) for tid, dmap in positional.items()}\n",
    "\n",
    "    return {\n",
    "        \"vocab\": vocab,            # term -> term_id\n",
    "        \"id2term\": id2term,        # term_id -> term\n",
    "        \"postings\": inv_index,     # term_id -> [doc_id, ...] (sorted)\n",
    "        \"doc_ids\": doc_ids,        # all doc ids, as strings\n",
    "        \"positional\": positional   # optional: term_id -> {doc_id: [positions]}\n",
    "    }\n",
    "\n",
    "index_obj = build_inverted_index_df(\n",
    "    df,\n",
    "    id_col=None,\n",
    "    text_cols=df.columns,\n",
    "    min_df=1,\n",
    "    store_positions=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddd7f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick lookups\n",
    "def docs_for_term(term: str):\n",
    "    \"\"\"Return document IDs for a raw term or categorical token (e.g., 'brand:nike').\"\"\"\n",
    "    tid = index_obj[\"vocab\"].get(term)\n",
    "    return index_obj[\"postings\"].get(tid, []) if tid is not None else []\n",
    "\n",
    "def doc_positions_for_term(term: str, doc_id: str):\n",
    "    \"\"\"Return positions of term in a specific document.\"\"\"\n",
    "    tid = index_obj[\"vocab\"].get(term)\n",
    "    if tid is None:\n",
    "        return []\n",
    "    return index_obj[\"positional\"].get(tid, {}).get(doc_id, [])\n",
    "\n",
    "def and_query(terms: list[str]):\n",
    "    \"\"\"Boolean AND over terms.\"\"\"\n",
    "    sets = [set(docs_for_term(t)) for t in terms]\n",
    "    return sorted(set.intersection(*sets)) if sets else []\n",
    "\n",
    "def or_query(terms: list[str]):\n",
    "    \"\"\"Boolean OR over terms.\"\"\"\n",
    "    s = set()\n",
    "    for t in terms:\n",
    "        s.update(docs_for_term(t))\n",
    "    return sorted(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b31c3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = {\n",
    "    \"Q1\": \"cotton tshirt 50 100 men blue\",\n",
    "    \"Q2\": \"adidas red\",\n",
    "    \"Q3\": \"denim jean skinny\",\n",
    "    \"Q4\": \"dress red\",\n",
    "    \"Q5\": \"leather jacket\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb0d2371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tf(word, document):\n",
    "    \"\"\"\n",
    "    Calculate term frequency for a word in a document.\n",
    "    TF = Number of times term t appears in a document\n",
    "    \"\"\"\n",
    "    return document.count(word)    \n",
    "    \n",
    "\n",
    "def calculate_idf(word, all_documents):\n",
    "    \"\"\"\n",
    "    Calculate inverse document frequency for a word.\n",
    "    IDF = log(Total number of documents / Number of documents containing term t)\n",
    "    \"\"\"\n",
    "    num_documents_with_term = len(docs_for_term(word))\n",
    "    if num_documents_with_term == 0:\n",
    "        return 0\n",
    "    return np.log(len(all_documents) / num_documents_with_term)\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors.\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
    "        return 0\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62a2ab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents(query, documents, k):\n",
    "    \"\"\"\n",
    "    Rank documents based on TF-IDF scores for the given query.\n",
    "    Return the top k documents.\n",
    "    \"\"\"\n",
    "    all_documents = [doc[\"description\"] + doc[\"title\"] + doc[\"brand\"] for index, doc in documents.iterrows()]\n",
    "    scores = []\n",
    "\n",
    "    term_idfs = {term: calculate_idf(term, all_documents) for term in query}\n",
    "    query_vector = np.array([calculate_tf(term, query) * term_idfs[term] for term in query])\n",
    "\n",
    "    for index, doc in documents.iterrows():\n",
    "        doc_vec = []\n",
    "        doc_text = doc[\"description\"] + doc[\"title\"] + doc[\"brand\"]\n",
    "        for term in query:\n",
    "            tf = calculate_tf(term, doc_text)\n",
    "            if tf > 0:\n",
    "                # used the formula tf = 1 + log_10(count)\n",
    "                doc_vec.append((1 + np.log(tf)) * term_idfs[term])\n",
    "            else:\n",
    "                doc_vec.append(0)\n",
    "        scores.append((doc, cosine_similarity(query_vector, np.array(doc_vec))))\n",
    "\n",
    "    # Sort documents by score in descending order\n",
    "    ranked_docs = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return ranked_docs[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08af6677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(retrieved_docs, relevant_docs, k):\n",
    "    \"\"\"\n",
    "    Calculate Precision@k.\n",
    "    Precision@k = (Number of relevant documents retrieved in top k) / k\n",
    "    \"\"\"\n",
    "    retrieved_at_k = retrieved_docs[:k]\n",
    "    relevant_retrieved = sum(1 for doc in retrieved_at_k if doc[\"pid\"] in relevant_docs)\n",
    "    return relevant_retrieved / k if k > 0 else 0\n",
    "\n",
    "def recall_at_k(retrieved_docs, relevant_docs, k):\n",
    "    \"\"\"\n",
    "    Calculate Recall@k.\n",
    "    Recall@k = (Number of relevant documents retrieved in top k) / (Total number of relevant documents)\n",
    "    \"\"\"\n",
    "    retrieved_at_k = retrieved_docs[:k]\n",
    "    relevant_retrieved = sum(1 for doc in retrieved_at_k if doc[\"pid\"] in relevant_docs)\n",
    "    total_relevant = len(relevant_docs)\n",
    "    return relevant_retrieved / total_relevant if total_relevant > 0 else 0\n",
    "\n",
    "def average_precision_at_k(retrieved_docs, relevant_docs, k):\n",
    "    \"\"\"\n",
    "    Calculate Average Precision@k.\n",
    "    AP@k = Average of Precision@i for each relevant document retrieved in top k\n",
    "    \"\"\"\n",
    "    retrieved_at_k = retrieved_docs[:k]\n",
    "    relevant_retrieved = 0\n",
    "    precision_sum = 0\n",
    "\n",
    "    for i, doc in enumerate(retrieved_at_k, start=1):\n",
    "        if doc[\"pid\"] in relevant_docs:\n",
    "            relevant_retrieved += 1\n",
    "            precision_sum += relevant_retrieved / i\n",
    "\n",
    "    return precision_sum / relevant_retrieved if relevant_retrieved > 0 else 0\n",
    "\n",
    "def f1_score(precision, recall):\n",
    "    \"\"\"\n",
    "    Calculate F1 Score.\n",
    "    F1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "    \"\"\"\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def f1_score_at_k(retrieved_docs, relevant_docs, k):\n",
    "    \"\"\"\n",
    "    Calculate F1 Score at k.\n",
    "    \"\"\"\n",
    "    precision = precision_at_k(retrieved_docs, relevant_docs, k)\n",
    "    recall = recall_at_k(retrieved_docs, relevant_docs, k)\n",
    "    return f1_score(precision, recall)\n",
    "\n",
    "def mean_average_precision(retrieved_docs_list, relevant_docs_list, k):\n",
    "    \"\"\"\n",
    "    Calculate Mean Average Precision (MAP) at k.\n",
    "    MAP = Mean of Average Precision@k over all queries\n",
    "    \"\"\"\n",
    "    ap_sum = 0\n",
    "    num_queries = len(retrieved_docs_list)\n",
    "\n",
    "    for retrieved_docs, relevant_docs in zip(retrieved_docs_list, relevant_docs_list):\n",
    "        ap_sum += average_precision_at_k(retrieved_docs, relevant_docs, k)\n",
    "\n",
    "    return ap_sum / num_queries if num_queries > 0 else 0\n",
    "\n",
    "def reciprocal_rank(retrieved_docs, relevant_docs):\n",
    "    \"\"\"\n",
    "    Calculate Reciprocal Rank (RR).\n",
    "    RR = 1 / Rank of the first relevant document\n",
    "    \"\"\"\n",
    "    rank = 0\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        if doc[\"pid\"] in relevant_docs:\n",
    "            rank = i + 1\n",
    "            break\n",
    "    return 1 / rank if rank > 0 else 0\n",
    "\n",
    "def mean_reciprocal_rank(retrieved_docs_list, relevant_docs_list):\n",
    "    \"\"\"\n",
    "    Calculate Mean Reciprocal Rank (MRR).\n",
    "    MRR = Mean of Reciprocal Ranks over all queries\n",
    "    \"\"\"\n",
    "    rr_sum = 0\n",
    "    num_queries = len(retrieved_docs_list)\n",
    "\n",
    "    for retrieved_docs, relevant_docs in zip(retrieved_docs_list, relevant_docs_list):\n",
    "        rr_sum += reciprocal_rank(retrieved_docs, relevant_docs)\n",
    "\n",
    "    return rr_sum / num_queries if num_queries > 0 else 0\n",
    "\n",
    "def dcg_at_k(retrieved_docs, relevant_docs, k):\n",
    "    \"\"\"\n",
    "    Calculate Discounted Cumulative Gain (DCG) at k.\n",
    "    DCG@k = Sum of (relevance of document at rank i) / log2(i + 1) for i in 1 to k\n",
    "    \"\"\"\n",
    "    dcg = 0\n",
    "    for i in range(min(k, len(retrieved_docs))):\n",
    "        doc = retrieved_docs[i]\n",
    "        if doc[\"pid\"] in relevant_docs:\n",
    "            relevance = 1  # we only have binary relevance\n",
    "        else:\n",
    "            relevance = 0\n",
    "        dcg += relevance / np.log2(i + 2)  # i + 2 because i starts from 0\n",
    "    return dcg\n",
    "\n",
    "def ndcg_at_k(retrieved_docs, relevant_docs, k):\n",
    "    \"\"\"\n",
    "    Calculate Normalized Discounted Cumulative Gain (NDCG) at k.\n",
    "    NDCG@k = DCG@k / IDCG@k\n",
    "    \"\"\"\n",
    "    dcg = dcg_at_k(retrieved_docs, relevant_docs, k)\n",
    "    \n",
    "    ideal_retrieved_docs = [{\"pid\": pid} for pid in relevant_docs]\n",
    "    idcg = dcg_at_k(ideal_retrieved_docs, relevant_docs, k)\n",
    "    \n",
    "    return dcg / idcg if idcg > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3_part3_title",
   "metadata": {},
   "source": [
    "## Ranking & Filtering\n",
    "\n",
    "In this part, we implement a retrieval pipeline that:\n",
    "- Takes a text query as input.\n",
    "- Finds all documents that contain all query terms (AND semantics).\n",
    "- Sorts the matching documents by relevance using multiple ranking methods:\n",
    "  1. TF‑IDF + cosine similarity\n",
    "  2. BM25\n",
    "  3. Our custom score (text relevance + numeric feature boosts)\n",
    "\n",
    "We also implement a Word2Vec + cosine ranking and return top 20 lists for the 5 queries defined in Part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3_prep_globals_md",
   "metadata": {},
   "source": [
    "### Global text corpus statistics for ranking\n",
    "We precompute text tokens per document (description + title + brand), document lengths, and per term document frequencies over these fields, which we will reuse for TF‑IDF and BM25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3_prep_globals",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a text-only view per document (description + title + brand)\n",
    "ALL_TEXT_DOCS = [row[\"description\"] + row[\"title\"] + row[\"brand\"] for _, row in df.iterrows()]\n",
    "N_TEXT = len(ALL_TEXT_DOCS)\n",
    "\n",
    "# Document lengths and average length\n",
    "DOC_LENGTHS = np.array([len(toks) for toks in ALL_TEXT_DOCS], dtype=float)\n",
    "AVG_DL = float(DOC_LENGTHS.mean()) if N_TEXT > 0 else 0.0\n",
    "\n",
    "# Document frequency per term over the text only view\n",
    "from collections import Counter\n",
    "DF_TEXT = Counter()\n",
    "for toks in ALL_TEXT_DOCS:\n",
    "    DF_TEXT.update(set(toks))\n",
    "\n",
    "def idf_text(term: str) -> float:\n",
    "    \"\"\"IDF using text only df: log(N / df)\"\"\"\n",
    "    df_t = DF_TEXT.get(term, 0)\n",
    "    if df_t == 0 or N_TEXT == 0:\n",
    "        return 0.0\n",
    "    return np.log(N_TEXT / df_t)\n",
    "\n",
    "def idf_bm25(term: str) -> float:\n",
    "    \"\"\"BM25 IDF: log((N - df + 0.5) / (df + 0.5) + 1)\"\"\"\n",
    "    df_t = DF_TEXT.get(term, 0)\n",
    "    return np.log(((N_TEXT - df_t + 0.5) / (df_t + 0.5)) + 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3_pipeline_md",
   "metadata": {},
   "source": [
    "### AND-conjunctive retrieval pipeline\n",
    "We standardize query preprocessing to match the document pipeline and use the inverted index to fetch conjunctive candidates before ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3_pipeline_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(q):\n",
    "    \"\"\"Accepts a string or list of strings; returns normalized tokens using the same preprocessing as documents\"\"\"\n",
    "    if isinstance(q, str):\n",
    "        return preprocess_text(q)\n",
    "    elif isinstance(q, (list, tuple)):\n",
    "        return preprocess_text(\" \".join(map(str, q)))\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def retrieve_conjunctive_candidates(query_tokens):\n",
    "    \"\"\"Return candidate doc indices (list of ints) where all query terms appear (AND)\"\"\"\n",
    "    if not query_tokens:\n",
    "        return []\n",
    "    dids = and_query(query_tokens)\n",
    "    return [int(d) for d in dids]\n",
    "\n",
    "def doc_text_tokens(row):\n",
    "    return row[\"description\"] + row[\"title\"] + row[\"brand\"]\n",
    "\n",
    "def cosine(v1, v2):\n",
    "    return cosine_similarity(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3_rankers_md",
   "metadata": {},
   "source": [
    "### Ranking methods\n",
    "We provide three ranking functions over the conjunctive candidates:\n",
    "- TF‑IDF + cosine similarity\n",
    "- BM25\n",
    "- Custom hybrid score (text relevance + numeric boosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3_rankers_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_tfidf_cosine(query_tokens, candidate_indices, k=10):\n",
    "    # Unique terms to define vector space\n",
    "    terms = list(dict.fromkeys(query_tokens))\n",
    "    # Query vector (tf-idf)\n",
    "    q_vec = np.array([(1 + np.log(query_tokens.count(t))) * idf_text(t) if query_tokens.count(t) > 0 else 0.0 for t in terms])\n",
    "\n",
    "    scored = []\n",
    "    for idx in candidate_indices:\n",
    "        row = df.iloc[idx]\n",
    "        toks = doc_text_tokens(row)\n",
    "        d_vec = []\n",
    "        for t in terms:\n",
    "            tf = toks.count(t)\n",
    "            d_vec.append(((1 + np.log(tf)) * idf_text(t)) if tf > 0 else 0.0)\n",
    "        d_vec = np.array(d_vec)\n",
    "        score = cosine(q_vec, d_vec)\n",
    "        scored.append((row, score))\n",
    "    return sorted(scored, key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "def rank_bm25(query_tokens, candidate_indices, k=10, k1=1.5, b=0.75):\n",
    "    scored = []\n",
    "    for idx in candidate_indices:\n",
    "        row = df.iloc[idx]\n",
    "        toks = doc_text_tokens(row)\n",
    "        dl = len(toks)\n",
    "        score = 0.0\n",
    "        for t in set(query_tokens):\n",
    "            tf = toks.count(t)\n",
    "            if tf == 0:\n",
    "                continue\n",
    "            idf = idf_bm25(t)\n",
    "            denom = tf + k1 * (1 - b + b * (dl / (AVG_DL if AVG_DL > 0 else 1.0)))\n",
    "            score += idf * (tf * (k1 + 1)) / denom\n",
    "        scored.append((row, score))\n",
    "    return sorted(scored, key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "# Global numeric ranges for normalization\n",
    "SELL_MIN, SELL_MAX = float(pd.to_numeric(df['selling_price'], errors='coerce').min()), float(pd.to_numeric(df['selling_price'], errors='coerce').max())\n",
    "DISC_MIN, DISC_MAX = float(pd.to_numeric(df['discount'], errors='coerce').min()), float(pd.to_numeric(df['discount'], errors='coerce').max())\n",
    "RAT_MIN, RAT_MAX = 0.0, 5.0  # ratings are on 0..5 scale\n",
    "\n",
    "def _norm(x, lo, hi):\n",
    "    try:\n",
    "        xv = float(x)\n",
    "    except Exception:\n",
    "        xv = 0.0\n",
    "    if hi <= lo:\n",
    "        return 0.0\n",
    "    return (xv - lo) / (hi - lo)\n",
    "\n",
    "def rank_custom_hybrid(query_tokens, candidate_indices, k=10,\n",
    "                       base='bm25', k1=1.5, b=0.75,\n",
    "                       w_rating=0.30, w_discount=0.20, w_price=0.10):\n",
    "    \"\"\"\n",
    "    Custom score = BaseTextScore * (1 + w_rating*rating_norm + w_discount*discount_norm - w_price*price_norm)\n",
    "    - BaseTextScore: BM25 (default) or TF‑IDF cosine\n",
    "    - Boosts: higher rating and discount help; higher price penalizes slightly\n",
    "    \"\"\"\n",
    "    # Precompute base scores once\n",
    "    if base == 'bm25':\n",
    "        base_scored = rank_bm25(query_tokens, candidate_indices, k=len(candidate_indices), k1=k1, b=b)\n",
    "    else:\n",
    "        base_scored = rank_tfidf_cosine(query_tokens, candidate_indices, k=len(candidate_indices))\n",
    "\n",
    "    out = []\n",
    "    for row, base_score in base_scored:\n",
    "        rating = 0.0 if pd.isna(row.get('average_rating', np.nan)) else float(row['average_rating'])\n",
    "        discount = row.get('discount', 0)\n",
    "        price = row.get('selling_price', 0)\n",
    "\n",
    "        rating_n = _norm(rating, RAT_MIN, RAT_MAX)\n",
    "        discount_n = _norm(discount, DISC_MIN, DISC_MAX)\n",
    "        price_n = _norm(price, SELL_MIN, SELL_MAX)\n",
    "\n",
    "        factor = 1.0 + (w_rating * rating_n) + (w_discount * discount_n) - (w_price * price_n)\n",
    "        final_score = float(base_score) * max(factor, 0.0)\n",
    "        out.append((row, final_score))\n",
    "\n",
    "    return sorted(out, key=lambda x: x[1], reverse=True)[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3_rankers_compare_md",
   "metadata": {},
   "source": [
    "### TF‑IDF vs BM25: Pros and Cons\n",
    "\n",
    "- TF‑IDF + cosine\n",
    "  - Pros: simple, fast, well understood; natural cosine normalization makes it robust to document length to some extent.\n",
    "  - Cons: raw tf grows unbounded and favors long documents; no saturation, so additional occurrences keep boosting; length normalization is implicit and weaker than BM25.\n",
    "\n",
    "- BM25\n",
    "  - Pros: tf saturation (diminishing returns); explicit length normalization with parameter b; strong and robust baseline in IR.\n",
    "  - Cons: requires hyperparameters (k1, b); scores are not normalized to [0,1] which can make mixing with other features less straightforward.\n",
    "\n",
    "Our custom hybrid score starts from a strong text base (BM25) and adds interpretable business signals: higher rating and bigger discount are preferred, while very high price is slightly penalized. This can better reflect user utility when text matches are similar. Downsides: requires choosing weights and assumes the same utility for all users (no personalization)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3_demo_rank_md",
   "metadata": {},
   "source": [
    "### Conjunctive retrieval + ranking (TF‑IDF, BM25, Custom)\n",
    "We run the 5 queries from Part 2 through the conjunctive filter and show the top‑10 pids for each ranking method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3_demo_rank_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_pid_title(scored, k=10):\n",
    "    out = []\n",
    "    for row, s in scored[:k]:\n",
    "        title_str = \" \".join(row['title']) if isinstance(row['title'], list) else str(row['title'])\n",
    "        out.append((row['pid'], round(float(s), 4), title_str))\n",
    "    return out\n",
    "\n",
    "demo_results = {}\n",
    "for qid, q_terms in test_queries.items():\n",
    "    q_tokens = preprocess_query(q_terms)\n",
    "    cand_idx = retrieve_conjunctive_candidates(q_tokens)\n",
    "    tfidf_res = rank_tfidf_cosine(q_tokens, cand_idx, k=10)\n",
    "    bm25_res = rank_bm25(q_tokens, cand_idx, k=10)\n",
    "    custom_res = rank_custom_hybrid(q_tokens, cand_idx, k=10, base='bm25')\n",
    "    demo_results[qid] = {\n",
    "        'TFIDF': top_k_pid_title(tfidf_res, 10),\n",
    "        'BM25': top_k_pid_title(bm25_res, 10),\n",
    "        'CUSTOM': top_k_pid_title(custom_res, 10)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3_demo_rank_print",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 TFIDF top-10: [('TSHFZKM8HSCZDQER', 0.2952, 'graphic print men round neck black dark blue tshirt pack 2'), ('TSHFZPENQKGHKYTU', 0.2952, 'graphic print men round neck dark blue red tshirt pack 2'), ('TSHFZQ3HHXFYTQJG', 0.2952, 'graphic print men round neck pink dark blue tshirt pack 2'), ('TSHFZQZAZD8MYBGZ', 0.2952, 'graphic print men round neck beig dark blue tshirt pack 2'), ('TSHFZQZAE9C5UTTD', 0.2952, 'graphic print men round neck white dark blue tshirt pack 2'), ('TSHFZ9JSB5MB9AAG', 0.2936, 'solid men v neck dark blue tshirt'), ('TSHFZ9K3AX2MBCPM', 0.2936, 'solid men round neck blue tshirt'), ('TSHFKKC5HS8TEJZW', 0.2936, 'stripe men polo neck blue tshirt'), ('TSHFYPGNHMKRUWAH', 0.2923, 'print men round neck dark blue tshirt'), ('TSHFYWFQMUCGYVUV', 0.2854, 'graphic print women round neck dark blue tshirt')]\n",
      "Q1 BM25 top-10: [('TSHFYPGNHMKRUWAH', 6.3481, 'print men round neck dark blue tshirt'), ('TSHFNWZUHE6PTYUG', 6.1676, 'typographi women round neck blue tshirt'), ('TSHFZ9K3AX2MBCPM', 6.1138, 'solid men round neck blue tshirt'), ('TSHFZ9JSB5MB9AAG', 6.0492, 'solid men v neck dark blue tshirt'), ('TSHFZKM8HSCZDQER', 5.7877, 'graphic print men round neck black dark blue tshirt pack 2'), ('TSHFZPENQKGHKYTU', 5.7877, 'graphic print men round neck dark blue red tshirt pack 2'), ('TSHFZQ3HHXFYTQJG', 5.7877, 'graphic print men round neck pink dark blue tshirt pack 2'), ('TSHFZQZAZD8MYBGZ', 5.7877, 'graphic print men round neck beig dark blue tshirt pack 2'), ('TSHFZQZAE9C5UTTD', 5.7877, 'graphic print men round neck white dark blue tshirt pack 2'), ('TSHFZ6DBE7GBBF2T', 5.6537, 'graphic print women round neck light blue tshirt')]\n",
      "Q1 CUSTOM top-10: [('TSHFYWFQMUCGYVUV', 7.2062, 'graphic print women round neck dark blue tshirt'), ('TSHFYWFQCPZUSTGX', 7.2062, 'graphic print women round neck dark blue tshirt'), ('TSHFYPGNHMKRUWAH', 7.0537, 'print men round neck dark blue tshirt'), ('TSHFNWZUHE6PTYUG', 6.853, 'typographi women round neck blue tshirt'), ('TSHFZ9K3AX2MBCPM', 6.803, 'solid men round neck blue tshirt'), ('TSHFZ9JSB5MB9AAG', 6.7292, 'solid men v neck dark blue tshirt'), ('TSHFH8HGZQYERVKF', 6.705, 'solid men polo neck red white blue tshirt pack 3'), ('TSHFUNN2PHXF7GUH', 6.4464, 'solid men round neck dark blue tshirt'), ('TSHFZKM8HSCZDQER', 6.4237, 'graphic print men round neck black dark blue tshirt pack 2'), ('TSHFZPENQKGHKYTU', 6.4237, 'graphic print men round neck dark blue red tshirt pack 2')]\n",
      "Q2 TFIDF top-10: [('TSHFZF6KF2GKH5ZM', 1.0, 'solid women round neck red tshirt'), ('TSHFZF6KAAJ3QBBZ', 0.9872, 'print women round neck red tshirt')]\n",
      "Q2 BM25 top-10: [('TSHFZF6KAAJ3QBBZ', 12.6267, 'print women round neck red tshirt'), ('TSHFZF6KF2GKH5ZM', 10.2671, 'solid women round neck red tshirt')]\n",
      "Q2 CUSTOM top-10: [('TSHFZF6KAAJ3QBBZ', 16.2653, 'print women round neck red tshirt'), ('TSHFZF6KF2GKH5ZM', 13.6554, 'solid women round neck red tshirt')]\n",
      "Q3 TFIDF top-10: [('TROFEQPUT59QKTKW', 1.0, 'taper women brown cotton linen blend trouser'), ('CRGFENCYZHHAK6PG', 1.0, 'men cargo'), ('TROFEQPUZ3MWMZE3', 1.0, 'taper women brown cotton linen blend trouser'), ('JEAEYNV6QZHJZFRH', 1.0, 'skinni women green jean'), ('JEAEYNV7Y7SGZAJB', 1.0, 'skinni men purpl jean'), ('JEAF6HFHG2FGKHBG', 1.0, 'skinni men green jean'), ('JEAF6HFHPSWA3FTS', 1.0, 'skinni women green jean'), ('JEAFMFFBMZZGCPYK', 0.9796, 'super skinni men black jean'), ('JEAFMFFBVRT3TBQM', 0.9796, 'super skinni women blue jean'), ('JEAFMFFBRXSUPJX5', 0.9796, 'skinni women blue jean')]\n",
      "Q3 BM25 top-10: [('JEAFWBJKAY9UHW8T', 14.3896, 'skinni women blue jean'), ('JEAFUZXSYS6GHGYH', 13.8389, 'skinni women blue jean'), ('JEAFUZXTDFARUHFR', 13.8389, 'skinni men blue jean'), ('JEAFUZXTFZGGMYFB', 13.8389, 'skinni women blue jean'), ('JEAFUZXSBBCAY7V6', 13.8389, 'skinni women grey jean'), ('JEAFUZXTHCYGZJFH', 13.8389, 'skinni men grey jean'), ('JEAFUZXS4AGG5HAQ', 13.8389, 'skinni women blue jean'), ('JEAFUZXTHR9UUXCG', 13.8389, 'skinni men blue jean'), ('JEAFUZXTBXFBREYS', 13.5789, 'skinni men blue jean'), ('JEAFXUHCWV9C5WNX', 13.5789, 'super skinni men blue jean')]\n",
      "Q3 CUSTOM top-10: [('JEAFWBJKAY9UHW8T', 16.8565, 'skinni women blue jean'), ('JEAFUZXTDFARUHFR', 16.3807, 'skinni men blue jean'), ('JEAEYNV6QZHJZFRH', 16.3655, 'skinni women green jean'), ('JEAFUZXTFZGGMYFB', 16.3163, 'skinni women blue jean'), ('JEAFUZXTHCYGZJFH', 16.2953, 'skinni men grey jean'), ('JEAFUZXSBBCAY7V6', 16.2883, 'skinni women grey jean'), ('JEAFUZXSYS6GHGYH', 16.2464, 'skinni women blue jean'), ('JEAFUZXS4AGG5HAQ', 16.2464, 'skinni women blue jean'), ('JEAFUZXTHR9UUXCG', 16.2464, 'skinni men blue jean'), ('JEAFXUHHTJNVG3PK', 16.1127, 'super skinni women blue jean')]\n",
      "Q4 TFIDF top-10: [('CTPFVPFV43SVFUWY', 1.0, 'nulit satin tie pin set red'), ('CTPFVPDDUYKBTAKH', 1.0, 'nulit satin tie pin set red'), ('CTPFVPDB593WABGP', 1.0, 'nulit satin tie pin set red'), ('CTPFVPGPW8DU3XJF', 1.0, 'nulit satin tie pin set red'), ('CTPFVZTBFX56AB3T', 1.0, 'nulit satin tie cufflink red'), ('TSHFGDEYFEY63WQA', 1.0, 'solid men polo neck red green black tshirt pack 3'), ('TSHFHBNTSG3UQWTV', 1.0, 'solid women polo neck dark blue red pink tshirt pack 3'), ('TSHFHHRKSYBQKPYF', 1.0, 'self design solid men polo neck red maroon pink tshirt pack 3'), ('TSHFGBY7MTBRY7GY', 1.0, 'solid women polo neck red green blue tshirt pack 3'), ('TSHFGBYHVCC5JFQU', 1.0, 'solid women polo neck light blue red blue tshirt pack 3')]\n",
      "Q4 BM25 top-10: [('SWTFVMSDDZRPXAPT', 7.3806, 'stripe round neck casual women revers red sweater'), ('KTAF5SD2XS8HWYCX', 5.53, 'men solid cotton blend straight kurta red'), ('KTAFD2UBNTQPBGAE', 5.4488, 'men embroid cotton blend straight kurta red'), ('KTAFD2UBNRWRAQJW', 5.4488, 'women embroid cotton blend straight kurta red'), ('KTAFD2UBSDPHPHXC', 5.4488, 'men embroid cotton blend straight kurta red'), ('SHOFGHWY36ZXNKMT', 5.0763, 'men red'), ('SHOFU4TFVFTAMNPD', 4.9411, 'parti wear women red'), ('CTPFVZTEMJWEJJJV', 3.9023, 'nulit satin tie cufflink red'), ('CTPFVZTBFX56AB3T', 3.4017, 'nulit satin tie cufflink red'), ('CTPFVZTBN4GRZKXH', 3.4017, 'nulit satin tie cufflink red')]\n",
      "Q4 CUSTOM top-10: [('KTAFD2UBNTQPBGAE', 7.5105, 'men embroid cotton blend straight kurta red'), ('KTAFD2UBNRWRAQJW', 7.5105, 'women embroid cotton blend straight kurta red'), ('KTAFD2UBSDPHPHXC', 7.5105, 'men embroid cotton blend straight kurta red'), ('SWTFVMSDDZRPXAPT', 7.4826, 'stripe round neck casual women revers red sweater'), ('KTAF5SD2XS8HWYCX', 7.1428, 'men solid cotton blend straight kurta red'), ('SHOFGHWY36ZXNKMT', 7.0001, 'men red'), ('SHOFU4TFVFTAMNPD', 6.4274, 'parti wear women red'), ('CTPFVZTEMJWEJJJV', 5.598, 'nulit satin tie cufflink red'), ('CTPFVZTBN4GRZKXH', 4.5924, 'nulit satin tie cufflink red'), ('CTPFVZTBFX56AB3T', 4.5646, 'nulit satin tie cufflink red')]\n",
      "Q5 TFIDF top-10: [('SHTFRSB3ZTK4GVEA', 1.0, 'women slim fit checker spread collar casual shirt'), ('JEAFVWFK6AFRNSFY', 1.0, 'skinni women dark blue jean'), ('SHTFSH5VRDYPSGGK', 1.0, 'women regular fit checker casual shirt'), ('JEAFMJHQHTYUAH7E', 1.0, 'taper fit women blue jean'), ('JEAFZZ996ZCSU2Z5', 1.0, 'slim women blue jean'), ('SHTFRTGXFZFH4PJE', 1.0, 'men regular fit checker casual shirt'), ('TSHFPHPHR5HFEVQT', 1.0, 'solid women henley neck black tshirt pack 3'), ('TSHFPHPHADJRNGSZ', 1.0, 'solid women henley neck dark blue green tshirt pack 3'), ('TSHFPHPHMFZHKSKH', 1.0, 'solid women henley neck dark blue red tshirt pack 3'), ('TSHFPHPHHDHVKJ9U', 1.0, 'solid men henley neck red tshirt pack 3')]\n",
      "Q5 BM25 top-10: [('JCKFWZBYQM2KQXCZ', 13.4896, 'full sleev solid women leather jacket'), ('JCKFWZBYEHTX3PFG', 13.4896, 'full sleev solid women leather jacket'), ('JCKFWZBYSXCEQDYD', 13.4896, 'full sleev solid men leather jacket'), ('JCKFWZBYVHJGQDGT', 13.4896, 'full sleev solid men leather jacket'), ('JCKFWZBYPVVGAHUR', 13.4896, 'full sleev solid women leather jacket'), ('JCKFWZBYHDRNMSZF', 13.4896, 'full sleev solid men leather jacket'), ('JCKFWZBYFHGWZ6RF', 13.4896, 'full sleev solid men leather jacket'), ('JCKFWZBYPATZVGPB', 13.4896, 'full sleev solid men leather jacket'), ('JCKFXY6FKFXW66DD', 12.3529, 'full sleev solid women leather jacket'), ('JCKFXY6FZMWUMTD4', 12.3529, 'full sleev solid men leather jacket')]\n",
      "Q5 CUSTOM top-10: [('JCKFWZBYQM2KQXCZ', 15.8746, 'full sleev solid women leather jacket'), ('JCKFWZBYEHTX3PFG', 15.8746, 'full sleev solid women leather jacket'), ('JCKFWZBYSXCEQDYD', 15.8746, 'full sleev solid men leather jacket'), ('JCKFWZBYVHJGQDGT', 15.8746, 'full sleev solid men leather jacket'), ('JCKFWZBYPVVGAHUR', 15.8746, 'full sleev solid women leather jacket'), ('JCKFWZBYHDRNMSZF', 15.8746, 'full sleev solid men leather jacket'), ('JCKFWZBYFHGWZ6RF', 15.8746, 'full sleev solid men leather jacket'), ('JCKFWZBYPATZVGPB', 15.8746, 'full sleev solid men leather jacket'), ('JCKFBFFZP9NHYH9Y', 15.2496, 'full sleev appliqu women leather jacket'), ('JCKFBFFP4GQJ3Y32', 14.82, 'full sleev color block appliqu women leather jacket')]\n"
     ]
    }
   ],
   "source": [
    "for qid, res in demo_results.items():\n",
    "    print(f\"{qid} TFIDF top-10:\", res['TFIDF'])\n",
    "    print(f\"{qid} BM25 top-10:\", res['BM25'])\n",
    "    print(f\"{qid} CUSTOM top-10:\", res['CUSTOM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3_w2v_md",
   "metadata": {},
   "source": [
    "## Word2Vec + cosine ranking\n",
    "We train a Word2Vec model on the corpus (description + title + brand tokens). A query or document is represented by averaging the vectors of its words. We then compute cosine similarity between the query vector and candidate document vectors.\n",
    "\n",
    "We return the top 20 documents for each of the 5 queries under AND semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3_w2v_train",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "w2v_dim = 100\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=ALL_TEXT_DOCS,\n",
    "    vector_size=w2v_dim,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    sg=1,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3_w2v_helpers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_w2v(tokens, model):\n",
    "    vecs = [model.wv[t] for t in tokens if t in model.wv]\n",
    "    if not vecs:\n",
    "        return None\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "def rank_w2v_cosine(query_tokens, candidate_indices, k=20):\n",
    "    q_vec = average_w2v(query_tokens, w2v_model)\n",
    "    if q_vec is None:\n",
    "        return []\n",
    "    scored = []\n",
    "    for idx in candidate_indices:\n",
    "        row = df.iloc[idx]\n",
    "        d_vec = average_w2v(doc_text_tokens(row), w2v_model)\n",
    "        if d_vec is None:\n",
    "            continue\n",
    "        score = cosine(q_vec, d_vec)\n",
    "        scored.append((row, score))\n",
    "    return sorted(scored, key=lambda x: x[1], reverse=True)[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3_w2v_run",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_results = {}\n",
    "for qid, q_terms in test_queries.items():\n",
    "    q_tokens = preprocess_query(q_terms)\n",
    "    cand_idx = retrieve_conjunctive_candidates(q_tokens)\n",
    "    w2v_scored = rank_w2v_cosine(q_tokens, cand_idx, k=20)\n",
    "    w2v_results[qid] = [(row['pid'], round(float(score), 4), \" \".join(row['title']) if isinstance(row['title'], list) else str(row['title'])) for row, score in w2v_scored]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3_w2v_print",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 by Word2Vec + cosine (conjunctive):\n",
      "Q1: [('TSHFH8HGZQYERVKF', 0.7993, 'solid men polo neck red white blue tshirt pack 3'), ('TSHFZQZAZD8MYBGZ', 0.7677, 'graphic print men round neck beig dark blue tshirt pack 2'), ('TSHFZQ3HHXFYTQJG', 0.766, 'graphic print men round neck pink dark blue tshirt pack 2'), ('TSHFZPENQKGHKYTU', 0.7635, 'graphic print men round neck dark blue red tshirt pack 2'), ('TSHFZKM8HSCZDQER', 0.7631, 'graphic print men round neck black dark blue tshirt pack 2'), ('TSHEU7DTUYMHMGW6', 0.7628, 'stripe women polo neck blue tshirt'), ('TSHFZ9JSB5MB9AAG', 0.762, 'solid men v neck dark blue tshirt'), ('TSHFZQZAE9C5UTTD', 0.761, 'graphic print men round neck white dark blue tshirt pack 2'), ('TSHFZ9K3AX2MBCPM', 0.7563, 'solid men round neck blue tshirt'), ('TSHFNWZUHE6PTYUG', 0.755, 'typographi women round neck blue tshirt'), ('TSHFUNN2PHXF7GUH', 0.7487, 'solid men round neck dark blue tshirt'), ('TSHFYPGNHMKRUWAH', 0.7459, 'print men round neck dark blue tshirt'), ('TSHFUNN3ZJEKSAEV', 0.7439, 'print men round neck dark blue tshirt'), ('TSHFZ6DBE7GBBF2T', 0.7418, 'graphic print women round neck light blue tshirt'), ('TSHFYWFQMUCGYVUV', 0.7396, 'graphic print women round neck dark blue tshirt'), ('TSHFYWFQCPZUSTGX', 0.7396, 'graphic print women round neck dark blue tshirt'), ('TSHFUNN2GGPB4PEH', 0.7355, 'print women round neck white blue tshirt'), ('TSHFPWKTUZVPHHNQ', 0.7051, 'print women round neck red tshirt'), ('TSHFZDP8MWWFXZFC', 0.6802, 'graphic print men round neck white tshirt'), ('TSHF5HGGCWY6XENZ', 0.6802, 'graphic print men round crew green tshirt')]\n",
      "Q2: [('TSHFZF6KAAJ3QBBZ', 0.8084, 'print women round neck red tshirt'), ('TSHFZF6KF2GKH5ZM', 0.7403, 'solid women round neck red tshirt')]\n",
      "Q3: [('JEAFZAUDSQBXBYWH', 0.8702, 'skinni men blue jean'), ('JEAFYA54APGZJ3CS', 0.8654, 'skinni women black jean'), ('JEAFNR7HYGPHGJEG', 0.8639, 'skinni men grey jean'), ('JEAFZJGGWHT8SDDN', 0.8632, 'skinni women grey jean'), ('JEAFG8BFYWZ8JSSP', 0.8286, 'skinni women dark blue jean'), ('JEAFG8BFHTD6SAQF', 0.8259, 'skinni women light blue jean'), ('JEAFUZ87WN7FWM8A', 0.7934, 'skinni women white jean'), ('JEAFWBJKAY9UHW8T', 0.767, 'skinni women blue jean'), ('JEAFUZXTHR9UUXCG', 0.7638, 'skinni men blue jean'), ('JEAFUZXSYS6GHGYH', 0.7631, 'skinni women blue jean'), ('JEAFUZXTFZGGMYFB', 0.7631, 'skinni women blue jean'), ('JEAFUZXS4AGG5HAQ', 0.7631, 'skinni women blue jean'), ('JEAFUZXTDFARUHFR', 0.7619, 'skinni men blue jean'), ('JEAFUZXSBBCAY7V6', 0.7571, 'skinni women grey jean'), ('JEAFXUHCWV9C5WNX', 0.757, 'super skinni men blue jean'), ('JEAFUZXTHCYGZJFH', 0.7559, 'skinni men grey jean'), ('JEAFXUHHTJNVG3PK', 0.7548, 'super skinni women blue jean'), ('JEAFXUHFWQTBJTNG', 0.7537, 'super skinni men blue jean'), ('JEAFXUHTQGBVANVY', 0.7537, 'super skinni men blue jean'), ('JEAFUZXTBXFBREYS', 0.7498, 'skinni men blue jean')]\n",
      "Q4: [('SWTFVMSDDZRPXAPT', 0.6096, 'stripe round neck casual women revers red sweater'), ('KTAF5SD2XS8HWYCX', 0.5436, 'men solid cotton blend straight kurta red'), ('KTAFD2UBNTQPBGAE', 0.5407, 'men embroid cotton blend straight kurta red'), ('KTAFD2UBSDPHPHXC', 0.5407, 'men embroid cotton blend straight kurta red'), ('KTAFD2UBNRWRAQJW', 0.5404, 'women embroid cotton blend straight kurta red'), ('SHTEMD8QKYNHW6HG', 0.5364, 'men regular fit solid regular collar formal shirt'), ('SHOFGHWY36ZXNKMT', 0.5119, 'men red'), ('SHOFU4TFVFTAMNPD', 0.5112, 'parti wear women red'), ('ETHF2557HFUJKF6S', 0.5108, 'men ethnic jacket kurta set cotton jute blend'), ('ETHF2557NWFC5AS6', 0.5108, 'men ethnic jacket kurta set cotton jute blend'), ('ETHF2557X6HKRUY3', 0.5106, 'women ethnic jacket kurta set cotton jute blend'), ('TSHFGDEYAGYNKSF8', 0.5087, 'solid women polo neck light blue green yellow tshirt pack 3'), ('CTPFVZTEMJWEJJJV', 0.5084, 'nulit satin tie cufflink red'), ('TSHFGDDYKKGHGJUY', 0.5077, 'solid women polo neck green light green yellow tshirt pack 3'), ('TSHFUWQAAECQFGJB', 0.502, 'solid women v neck maroon tshirt'), ('TSHFUXPWUR2ZTEE2', 0.5005, 'color block men v neck dark blue tshirt'), ('TSHF4PFQK67YGUY7', 0.4982, 'solid men round neck white tshirt'), ('TSHFGDDWWRXA3ESX', 0.4938, 'solid women polo neck green maroon orang tshirt pack 3'), ('TSHFSA79BXZUERYB', 0.4908, 'print men round neck yellow tshirt'), ('TSHFSA79NFYWPMWT', 0.4908, 'print men round neck yellow tshirt')]\n",
      "Q5: [('JCKFWZBYQM2KQXCZ', 0.8127, 'full sleev solid women leather jacket'), ('JCKFWZBYEHTX3PFG', 0.8127, 'full sleev solid women leather jacket'), ('JCKFWZBYPVVGAHUR', 0.8127, 'full sleev solid women leather jacket'), ('JCKFWZBYSXCEQDYD', 0.8123, 'full sleev solid men leather jacket'), ('JCKFWZBYVHJGQDGT', 0.8123, 'full sleev solid men leather jacket'), ('JCKFWZBYHDRNMSZF', 0.8123, 'full sleev solid men leather jacket'), ('JCKFWZBYFHGWZ6RF', 0.8123, 'full sleev solid men leather jacket'), ('JCKFWZBYPATZVGPB', 0.8123, 'full sleev solid men leather jacket'), ('JCKF7FHYECSZGCQA', 0.7317, 'full sleev solid men quilt jacket'), ('JCKFAYVDMYZEAMUY', 0.7202, 'full sleev appliqu women leather jacket'), ('JCKFBFFZP9NHYH9Y', 0.7182, 'full sleev appliqu women leather jacket'), ('JCKFXY6FKFXW66DD', 0.7173, 'full sleev solid women leather jacket'), ('JCKFXY6F9UFABRGP', 0.7173, 'full sleev solid women leather jacket'), ('JCKFXY6FHKFVSJEZ', 0.7173, 'full sleev solid women leather jacket'), ('JCKFXY6FPFZTG5WN', 0.7173, 'full sleev solid women leather jacket'), ('JCKFXY6FPHVHGFG4', 0.7173, 'full sleev solid women leather jacket'), ('JCKFXY6FZMWUMTD4', 0.716, 'full sleev solid men leather jacket'), ('JCKFXY6FC8Z6YENR', 0.716, 'full sleev solid men leather jacket'), ('JCKFXY6FKF36GMNN', 0.716, 'full sleev solid men leather jacket'), ('JCKFAYVDVBJJSU2K', 0.7097, 'full sleev color block appliqu men leather jacket')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 20 by Word2Vec + cosine (conjunctive):\")\n",
    "for qid in [\"Q1\",\"Q2\",\"Q3\",\"Q4\",\"Q5\"]:\n",
    "    print(f\"{qid}:\", w2v_results.get(qid, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f440770-a824-4775-aca1-509227910363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_PATH = \"../../data/fashion_products_dataset.json\"\n",
    "OPENAI_API_KEY = \"\" # We need an api key somewhere for openai bc for api you must have billing which I don't have\n",
    "#✨ AI Summary Error generating summary: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please \n",
    "#check your plan and billing details. For more information on this error, read the docs: \n",
    "#https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de237f0e-35ad-4381-beca-9ca72f2d58e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Engine Initialized.\n"
     ]
    }
   ],
   "source": [
    "class SearchEngine:\n",
    "    def __init__(self, file_path):\n",
    "        self.df = self.load_and_clean_data(file_path)\n",
    "        self.index = self.build_index()\n",
    "        self.doc_lengths = self.compute_doc_lengths()\n",
    "        self.avg_dl = float(np.mean(self.doc_lengths))\n",
    "        self.N = len(self.df)\n",
    "        print(\"Search Engine Initialized.\")\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        if not isinstance(text, str): return []\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [t.lower() for t in tokens]\n",
    "        tokens = [re.sub(r\"[^\\w\\s]\", \"\", t) for t in tokens]\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [t for t in tokens if t not in stop_words and t]\n",
    "        stemmer = PorterStemmer()\n",
    "        return [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "    def load_and_clean_data(self, path):\n",
    "        with open(path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        def clean_row(row):\n",
    "            # 1. Prices\n",
    "            try:\n",
    "                sp = float(str(row['selling_price']).replace(',','')) if row['selling_price'] else 0.0\n",
    "            except: sp = 0.0\n",
    "            \n",
    "            disc = 0\n",
    "            if isinstance(row['discount'], str) and '%' in row['discount']:\n",
    "                 disc = int(re.search(r'\\d+', row['discount']).group())\n",
    "            \n",
    "            ap = row['actual_price']\n",
    "            if not ap:\n",
    "                ap = sp * (1 + disc/100) if disc > 0 else sp\n",
    "            else:\n",
    "                try: ap = float(str(ap).replace(',',''))\n",
    "                except: ap = sp\n",
    "\n",
    "            # 2. Text\n",
    "            row['processed_title'] = self.preprocess_text(row['title'])\n",
    "            row['processed_desc'] = self.preprocess_text(row['description'])\n",
    "            brand = row['brand'] if row['brand'] else \"no brand\"\n",
    "            row['processed_brand'] = [brand.lower()]\n",
    "            \n",
    "            # Combine for indexing\n",
    "            row['all_tokens'] = row['processed_title'] + row['processed_desc'] + row['processed_brand']\n",
    "            \n",
    "            # 3. Numeric Types for sorting\n",
    "            row['selling_price_val'] = sp\n",
    "            row['actual_price_val'] = ap\n",
    "            row['discount_val'] = disc\n",
    "            try: row['rating_val'] = float(row['average_rating'])\n",
    "            except: row['rating_val'] = 0.0\n",
    "            \n",
    "            return row\n",
    "\n",
    "        df = df.apply(clean_row, axis=1)\n",
    "        # Generate a unique string ID for URLs if pid is missing or complex\n",
    "        df['uid'] = [str(uuid.uuid4()) for _ in range(len(df))] \n",
    "        return df\n",
    "\n",
    "    def build_index(self):\n",
    "        inv_index = defaultdict(list)\n",
    "        for idx, row in self.df.iterrows():\n",
    "            # Use set for boolean retrieval to avoid duplicates per doc\n",
    "            for term in set(row['all_tokens']):\n",
    "                inv_index[term].append(idx)\n",
    "        return inv_index\n",
    "\n",
    "    def compute_doc_lengths(self):\n",
    "        return np.array([len(tokens) for tokens in self.df['all_tokens']])\n",
    "\n",
    "    def get_idf(self, term):\n",
    "        df_count = len(self.index.get(term, []))\n",
    "        return np.log(1 + (self.N - df_count + 0.5) / (df_count + 0.5))\n",
    "\n",
    "    def search(self, query, k=20):\n",
    "        \"\"\"\n",
    "        Implements BM25 + Boosting (Your 'Custom' logic).\n",
    "        \"\"\"\n",
    "        q_tokens = self.preprocess_text(query)\n",
    "        \n",
    "        # 1. Retrieve (Conjunctive OR to ensure recall, ranked by BM25)\n",
    "        doc_scores = defaultdict(float)\n",
    "        k1, b = 1.5, 0.75\n",
    "        \n",
    "        relevant_indices = set()\n",
    "        for t in q_tokens:\n",
    "            relevant_indices.update(self.index.get(t, []))\n",
    "            \n",
    "        if not relevant_indices:\n",
    "            return []\n",
    "\n",
    "        for idx in relevant_indices:\n",
    "            doc_tokens = self.df.iloc[idx]['all_tokens']\n",
    "            dl = len(doc_tokens)\n",
    "            score = 0\n",
    "            for t in q_tokens:\n",
    "                tf = doc_tokens.count(t) # Note: In prod, pre-calculate TF\n",
    "                if tf > 0:\n",
    "                    idf = self.get_idf(t)\n",
    "                    num = tf * (k1 + 1)\n",
    "                    den = tf + k1 * (1 - b + b * (dl / self.avg_dl))\n",
    "                    score += idf * (num / den)\n",
    "            \n",
    "            # 2. Custom Boosting (Rating & Discount)\n",
    "            row = self.df.iloc[idx]\n",
    "            boost = 1.0\n",
    "            if row['rating_val'] > 4.0: boost += 0.2\n",
    "            if row['discount_val'] > 30: boost += 0.1\n",
    "            \n",
    "            doc_scores[idx] = score * boost\n",
    "\n",
    "        sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "        \n",
    "        results = []\n",
    "        for idx, score in sorted_docs:\n",
    "            results.append(self.df.iloc[idx].to_dict())\n",
    "            results[-1]['search_score'] = score\n",
    "            \n",
    "        return results\n",
    "\n",
    "# Initialize Engine\n",
    "engine = SearchEngine(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f206e8c1-f412-469d-9d1f-4c1e0adbf64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "class RAGSystem:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        self.client = None\n",
    "        \n",
    "        if api_key != \"YOUR_OPENAI_API_KEY\":\n",
    "            try:\n",
    "                self.client = OpenAI(api_key=api_key)\n",
    "            except Exception as e:\n",
    "                print(f\"Error initializing OpenAI Client: {e}\")\n",
    "\n",
    "    def generate_summary(self, query, top_results):\n",
    "        \"\"\"\n",
    "        Improvement 1: Context Pruning. Only take top 5 results.\n",
    "        Improvement 2: Metadata Injection. Include price/rating in context.\n",
    "        \"\"\"\n",
    "        if not top_results:\n",
    "            return \"No products found to summarize.\"\n",
    "\n",
    "        context_text = \"\"\n",
    "        for i, res in enumerate(top_results[:5]): # Only top 5\n",
    "            context_text += f\"Item {i+1}: {res['title']}. Price: {res['selling_price']}. Rating: {res['average_rating']}. Description: {res['description'][:200]}...\\n\"\n",
    "\n",
    "        # Improvement 3: Refined Prompt\n",
    "        prompt = f\"\"\"\n",
    "        You are an expert Fashion Shopping Assistant. \n",
    "        User Query: \"{query}\"\n",
    "        \n",
    "        Based ONLY on the following available products, provide a 3-sentence summary helping the user choose. \n",
    "        Highlight the best value (high rating + low price) if it exists.\n",
    "        \n",
    "        Products:\n",
    "        {context_text}\n",
    "        \n",
    "        Summary:\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            if self.api_key == \"YOUR_OPENAI_API_KEY\":\n",
    "                return \"LLM Summary Placeholder: Please insert a valid OpenAI API Key in Cell 1 to see the generated text. (Simulating: These products match your query with various price points...)\"\n",
    "            \n",
    "            if self.client is None:\n",
    "                return \"Error: OpenAI Client not initialized. Check your API Key.\"\n",
    "\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=150\n",
    "            )\n",
    "            # Access response attributes (not dictionary keys)\n",
    "            return response.choices[0].message.content.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generating summary: {str(e)}\"\n",
    "\n",
    "# Re-initialize the RAG system\n",
    "rag = RAGSystem(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a6c6695-08e6-4a24-8ba1-c981d0ece729",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalyticsStore:\n",
    "    def __init__(self):\n",
    "        # 1. Sessions (User Context)\n",
    "        self.sessions = [] # [{'session_id', 'user_agent', 'timestamp', 'ip'}]\n",
    "        \n",
    "        # 2. Requests (Search Queries)\n",
    "        self.searches = [] # [{'search_id', 'session_id', 'query', 'timestamp', 'num_results'}]\n",
    "        \n",
    "        # 3. Clicks (Interaction)\n",
    "        self.clicks = []   # [{'session_id', 'search_id', 'doc_uid', 'rank', 'timestamp', 'dwell_time'}]\n",
    "\n",
    "    def track_session(self, user_agent, ip):\n",
    "        sid = str(uuid.uuid4())\n",
    "        self.sessions.append({\n",
    "            'session_id': sid,\n",
    "            'user_agent': user_agent,\n",
    "            'ip': ip,\n",
    "            'timestamp': datetime.datetime.now().isoformat()\n",
    "        })\n",
    "        return sid\n",
    "\n",
    "    def track_search(self, session_id, query, num_results):\n",
    "        search_id = str(uuid.uuid4())\n",
    "        self.searches.append({\n",
    "            'search_id': search_id,\n",
    "            'session_id': session_id,\n",
    "            'query': query,\n",
    "            'num_results': num_results,\n",
    "            'timestamp': datetime.datetime.now().isoformat()\n",
    "        })\n",
    "        return search_id\n",
    "\n",
    "    def track_click(self, session_id, search_id, doc_uid, rank):\n",
    "        self.clicks.append({\n",
    "            'session_id': session_id,\n",
    "            'search_id': search_id,\n",
    "            'doc_uid': doc_uid,\n",
    "            'rank': rank,\n",
    "            'timestamp': datetime.datetime.now().isoformat(),\n",
    "            'dwell_time': 0 # Updated later via Beacon\n",
    "        })\n",
    "\n",
    "    def update_dwell_time(self, session_id, doc_uid, duration):\n",
    "        # Find the most recent click for this session/doc and update\n",
    "        for click in reversed(self.clicks):\n",
    "            if click['session_id'] == session_id and click['doc_uid'] == doc_uid:\n",
    "                click['dwell_time'] = duration\n",
    "                break\n",
    "\n",
    "    def get_stats(self):\n",
    "        total_searches = len(self.searches)\n",
    "        total_clicks = len(self.clicks)\n",
    "        \n",
    "        # Top Queries\n",
    "        queries = [s['query'] for s in self.searches]\n",
    "        top_queries = Counter(queries).most_common(5)\n",
    "        \n",
    "        # Click Through Rate (CTR)\n",
    "        ctr = (total_clicks / total_searches * 100) if total_searches > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'total_searches': total_searches,\n",
    "            'total_clicks': total_clicks,\n",
    "            'ctr': round(ctr, 2),\n",
    "            'top_queries': top_queries,\n",
    "            'raw_sessions': self.sessions[-5:], # Last 5\n",
    "            'raw_clicks': self.clicks[-5:]\n",
    "        }\n",
    "\n",
    "analytics = AnalyticsStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "911639cf-3585-4ee7-9a6d-e1b931b5846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CSS STYLES ---\n",
    "CSS = \"\"\"\n",
    "<style>\n",
    "    body { font-family: 'Segoe UI', sans-serif; max-width: 1000px; margin: 0 auto; padding: 20px; background-color: #f9f9f9; }\n",
    "    .header { text-align: center; margin-bottom: 40px; }\n",
    "    .search-box { display: flex; justify-content: center; gap: 10px; margin-bottom: 30px; }\n",
    "    input[type=\"text\"] { width: 60%; padding: 12px; border: 1px solid #ddd; border-radius: 4px; font-size: 16px; }\n",
    "    button { padding: 12px 24px; background-color: #333; color: white; border: none; border-radius: 4px; cursor: pointer; }\n",
    "    button:hover { background-color: #555; }\n",
    "    \n",
    "    .rag-box { background: #e8f4fd; border: 1px solid #b6dbf9; padding: 15px; border-radius: 8px; margin-bottom: 20px; }\n",
    "    .rag-title { font-weight: bold; color: #0056b3; margin-bottom: 5px; }\n",
    "    \n",
    "    .result-item { background: white; padding: 20px; margin-bottom: 15px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.05); }\n",
    "    .result-title a { font-size: 18px; color: #333; text-decoration: none; font-weight: bold; }\n",
    "    .result-meta { color: #666; font-size: 14px; margin: 5px 0; }\n",
    "    .price { color: #d9534f; font-weight: bold; }\n",
    "    \n",
    "    .dashboard-grid { display: grid; grid-template-columns: 1fr 1fr; gap: 20px; }\n",
    "    .card { background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }\n",
    "    table { width: 100%; border-collapse: collapse; }\n",
    "    th, td { padding: 8px; text-align: left; border-bottom: 1px solid #ddd; }\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "# --- TEMPLATES ---\n",
    "\n",
    "HOME_TEMPLATE = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head><title>Fashion Search</title>\"\"\" + CSS + \"\"\"</head>\n",
    "<body>\n",
    "    <div class=\"header\">\n",
    "        <h1>Fashion Search Engine</h1>\n",
    "        <p>Find the best clothes with AI</p>\n",
    "    </div>\n",
    "    <form action=\"/search\" method=\"get\" class=\"search-box\">\n",
    "        <input type=\"text\" name=\"q\" placeholder=\"e.g. Red cotton summer dress\" required>\n",
    "        <button type=\"submit\">Search</button>\n",
    "    </form>\n",
    "    <div style=\"text-align:center;\">\n",
    "        <a href=\"/dashboard\">View Analytics Dashboard</a>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "RESULTS_TEMPLATE = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head><title>Results for {{ query }}</title>\"\"\" + CSS + \"\"\"</head>\n",
    "<body>\n",
    "    <div class=\"header\">\n",
    "        <form action=\"/search\" method=\"get\" class=\"search-box\">\n",
    "            <input type=\"text\" name=\"q\" value=\"{{ query }}\">\n",
    "            <button type=\"submit\">Search</button>\n",
    "        </form>\n",
    "    </div>\n",
    "\n",
    "    <div class=\"rag-box\">\n",
    "        <div class=\"rag-title\">AI Summary</div>\n",
    "        <div>{{ summary }}</div>\n",
    "    </div>\n",
    "\n",
    "    {% for doc in results %}\n",
    "    <div class=\"result-item\">\n",
    "        <div class=\"result-title\">\n",
    "            <a href=\"/product/{{ doc.uid }}?sid={{ session_id }}&qid={{ search_id }}&rank={{ loop.index }}\" \n",
    "               onclick=\"trackClick('{{ session_id }}', '{{ search_id }}', '{{ doc.uid }}', {{ loop.index }})\">\n",
    "               {{ doc.title }}\n",
    "            </a>\n",
    "        </div>\n",
    "        <div class=\"result-meta\">\n",
    "            <span class=\"price\">{{ doc.selling_price }}</span> \n",
    "            {% if doc.discount_val > 0 %} <span style=\"color:green\">({{ doc.discount }} off)</span> {% endif %}\n",
    "            | Rating: {{ doc.average_rating }} ★ | Brand: {{ doc.brand }}\n",
    "        </div>\n",
    "        <div style=\"color:#555;\">{{ doc.description[:150] }}...</div>\n",
    "    </div>\n",
    "    {% endfor %}\n",
    "    \n",
    "    <script>\n",
    "    function trackClick(sid, qid, docId, rank) {\n",
    "        // We use the href for navigation, but we could fire an async fetch here if we prevented default\n",
    "        // The simple link with query params handles the tracking on the server side /product route\n",
    "    }\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "DETAILS_TEMPLATE = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head><title>{{ doc.title }}</title>\"\"\" + CSS + \"\"\"</head>\n",
    "<body>\n",
    "    <a href=\"javascript:history.back()\">← Back to results</a>\n",
    "    \n",
    "    <div class=\"card\" style=\"margin-top:20px;\">\n",
    "        <h1>{{ doc.title }}</h1>\n",
    "        <h2 class=\"price\">{{ doc.selling_price }}</h2>\n",
    "        <p><strong>Brand:</strong> {{ doc.brand }}</p>\n",
    "        <p><strong>Rating:</strong> {{ doc.average_rating }} / 5.0</p>\n",
    "        <hr>\n",
    "        <h3>Description</h3>\n",
    "        <p>{{ doc.description }}</p>\n",
    "        <br>\n",
    "        <h3>Product Details</h3>\n",
    "        <p>{{ doc.product_details }}</p>\n",
    "        <br>\n",
    "        <a href=\"{{ doc.url }}\" target=\"_blank\" style=\"background:black; color:white; padding:10px 20px; text-decoration:none; border-radius:4px;\">Buy on Original Site</a>\n",
    "    </div>\n",
    "\n",
    "    <script>\n",
    "        // WEB ANALYTICS: Dwell Time Tracking\n",
    "        let startTime = Date.now();\n",
    "        let sid = \"{{ session_id }}\";\n",
    "        let docUid = \"{{ doc.uid }}\";\n",
    "\n",
    "        window.addEventListener(\"beforeunload\", function() {\n",
    "            let endTime = Date.now();\n",
    "            let duration = (endTime - startTime) / 1000; // seconds\n",
    "            \n",
    "            // Send beacon (reliable on page unload)\n",
    "            navigator.sendBeacon(\"/track_dwell\", JSON.stringify({\n",
    "                session_id: sid,\n",
    "                doc_uid: docUid,\n",
    "                duration: duration\n",
    "            }));\n",
    "        });\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "DASHBOARD_TEMPLATE = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Analytics Dashboard</title>\n",
    "    \"\"\" + CSS + \"\"\"\n",
    "    <script src=\"https://cdn.jsdelivr.net/npm/chart.js\"></script>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Web Analytics Dashboard</h1>\n",
    "    <a href=\"/\">← Back to Search</a>\n",
    "    <br><br>\n",
    "\n",
    "    <div class=\"dashboard-grid\">\n",
    "        <div class=\"card\">\n",
    "            <h3>Key Metrics</h3>\n",
    "            <p><strong>Total Searches:</strong> {{ stats.total_searches }}</p>\n",
    "            <p><strong>Total Clicks:</strong> {{ stats.total_clicks }}</p>\n",
    "            <p><strong>CTR:</strong> {{ stats.ctr }}%</p>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"card\">\n",
    "            <h3>Top Queries</h3>\n",
    "            <ul>\n",
    "            {% for q, count in stats.top_queries %}\n",
    "                <li>{{ q }} ({{ count }})</li>\n",
    "            {% endfor %}\n",
    "            </ul>\n",
    "        </div>\n",
    "    </div>\n",
    "    \n",
    "    <br>\n",
    "    \n",
    "    <div class=\"card\">\n",
    "        <h3>Recent Interactions (Log)</h3>\n",
    "        <table>\n",
    "            <tr><th>Time</th><th>Type</th><th>Detail</th></tr>\n",
    "            {% for c in stats.raw_clicks reversed %}\n",
    "            <tr>\n",
    "                <td>{{ c.timestamp }}</td>\n",
    "                <td>CLICK</td>\n",
    "                <td>Rank {{ c.rank }} (Dwell: {{ c.dwell_time|round(1) }}s)</td>\n",
    "            </tr>\n",
    "            {% endfor %}\n",
    "            {% for s in stats.raw_sessions reversed %}\n",
    "            <tr>\n",
    "                <td>{{ s.timestamp }}</td>\n",
    "                <td>SESSION</td>\n",
    "                <td>{{ s.user_agent[:30] }}...</td>\n",
    "            </tr>\n",
    "            {% endfor %}\n",
    "        </table>\n",
    "    </div>\n",
    "\n",
    "    \n",
    "\n",
    "[Image of Analytics Architecture Diagram]\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0dcb8aa0-1bad-4a6b-89c0-7e5bb58bdc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Flask Server...\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.1.148:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [01/Dec/2025 22:56:29] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [01/Dec/2025 22:56:42] \"GET /search?q=red+trousers+men HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [01/Dec/2025 22:57:03] \"GET /search?q=men+boxers+black HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.before_request\n",
    "def ensure_session():\n",
    "    # Simple session tracking via cookie logic (simplified)\n",
    "    pass\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    # Start a session\n",
    "    sid = analytics.track_session(request.headers.get('User-Agent'), request.remote_addr)\n",
    "    return render_template_string(HOME_TEMPLATE)\n",
    "\n",
    "@app.route('/search')\n",
    "def search():\n",
    "    query = request.args.get('q', '')\n",
    "    \n",
    "    # 1. Analytics: Get or Create Session\n",
    "    sid = analytics.track_session(request.headers.get('User-Agent'), request.remote_addr)\n",
    "    \n",
    "    # 2. Search Engine\n",
    "    results = engine.search(query, k=15)\n",
    "    \n",
    "    # 3. Analytics: Track Search\n",
    "    search_id = analytics.track_search(sid, query, len(results))\n",
    "    \n",
    "    # 4. RAG: Generate Summary\n",
    "    summary = rag.generate_summary(query, results)\n",
    "    \n",
    "    return render_template_string(\n",
    "        RESULTS_TEMPLATE, \n",
    "        query=query, \n",
    "        results=results, \n",
    "        summary=summary,\n",
    "        session_id=sid,\n",
    "        search_id=search_id\n",
    "    )\n",
    "\n",
    "@app.route('/product/<uid>')\n",
    "def product_detail(uid):\n",
    "    # Retrieve query params for analytics\n",
    "    sid = request.args.get('sid', 'unknown')\n",
    "    qid = request.args.get('qid', 'unknown')\n",
    "    rank = request.args.get('rank', 0)\n",
    "    \n",
    "    # 1. Analytics: Track Click\n",
    "    analytics.track_click(sid, qid, uid, rank)\n",
    "    \n",
    "    # 2. Find Document\n",
    "    doc = engine.df[engine.df['uid'] == uid].iloc[0].to_dict()\n",
    "    \n",
    "    return render_template_string(\n",
    "        DETAILS_TEMPLATE, \n",
    "        doc=doc, \n",
    "        session_id=sid\n",
    "    )\n",
    "\n",
    "@app.route('/track_dwell', methods=['POST'])\n",
    "def track_dwell():\n",
    "    # Endpoint for Beacon API\n",
    "    data = json.loads(request.data)\n",
    "    analytics.update_dwell_time(data['session_id'], data['doc_uid'], data['duration'])\n",
    "    return jsonify({\"status\": \"success\"})\n",
    "\n",
    "@app.route('/dashboard')\n",
    "def dashboard():\n",
    "    stats = analytics.get_stats()\n",
    "    return render_template_string(DASHBOARD_TEMPLATE, stats=stats)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Run the server\n",
    "    print(\"Starting Flask Server...\")\n",
    "    app.run(host='0.0.0.0', port=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
