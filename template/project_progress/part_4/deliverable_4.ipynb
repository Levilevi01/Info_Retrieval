{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38d2ad7a",
   "metadata": {},
   "source": [
    "# Deliverable 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2fbece",
   "metadata": {},
   "source": [
    "SNumbers: u264332, u264443, u264202\n",
    "\n",
    "Names: Levente Olivér Bódi, Riccardo Zamuner, Giada Izzo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b3b603",
   "metadata": {},
   "source": [
    "## Previous deliverable code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd734c6",
   "metadata": {},
   "source": [
    "This is the same code of the previous deliverable minus prints and commentary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17d228d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import uuid\n",
    "import datetime\n",
    "from flask import Flask, request, render_template_string, jsonify\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65b9f84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess a text by tokenizing, lowercasing, removing stop words, and stemming.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # Remove punctuation\n",
    "    tokens = [re.sub(r\"[^\\w\\s]\", \"\", token) for token in tokens]\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Remove punctuation\n",
    "    tokens = [re.sub(r\"[^\\w\\s]\", \"\", token) for token in tokens]\n",
    "\n",
    "    # Stem the tokens\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    # Remove empty strings\n",
    "    tokens = [token for token in tokens if token]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def clean_seller(text):\n",
    "    \"\"\"\n",
    "    Clean the seller field by removing unwanted trailing phrases.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove unwanted trailing phrases and everything after them\n",
    "    remove_phrases = [\n",
    "        \"Seller changed\",\n",
    "        \"(Not Enough Ratin\",\n",
    "        \"(New Sell\"\n",
    "    ]\n",
    "    for phrase in remove_phrases:\n",
    "        idx = text.find(phrase)\n",
    "        if idx != -1:\n",
    "            text = text[:idx]\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_non_textual(document):\n",
    "    \"\"\"\n",
    "    Preprocess non-textual fields in the document.\n",
    "    \"\"\"\n",
    "\n",
    "    # Discount preprocessing: convert from string \"xx% off\" to integer xx\n",
    "    # also taking into account documents without discount\n",
    "    if isinstance(document[\"discount\"], str) and \"%\" in document[\"discount\"]:\n",
    "        document[\"discount\"] = int(document[\"discount\"][:document[\"discount\"].find(\"%\")])\n",
    "    else:\n",
    "        document[\"discount\"] = 0\n",
    "        \n",
    "    # Merge all values from product_details dictionary and preprocess\n",
    "    if isinstance(document[\"product_details\"], dict):\n",
    "        details_text = \" \".join(str(v) for v in document[\"product_details\"].values())\n",
    "    elif isinstance(document[\"product_details\"], list):\n",
    "        # If it's a list of dicts, merge all values from all dicts\n",
    "        details_text = \" \".join(str(v) for d in document[\"product_details\"] if isinstance(d, dict) for v in d.values())\n",
    "    else:\n",
    "        details_text = str(document[\"product_details\"])\n",
    "    document[\"product_details\"] = preprocess_text(details_text)\n",
    "\n",
    "    # Convert actual_price and selling_price to integers (remove commas)\n",
    "    # If actual_price is NaN, set it to discounted selling_price\n",
    "    for price_field in [\"actual_price\", \"selling_price\"]:\n",
    "        if isinstance(document[price_field], str):\n",
    "            price_str = document[price_field].replace(\",\", \"\")\n",
    "            price_val = price_str.split(\".\")[0]\n",
    "            document[price_field] = int(price_val) if price_val.isdigit() else 0\n",
    "\n",
    "    # If actual_price is missing or zero, set it to discounted selling_price\n",
    "    if (\"actual_price\" not in document or document[\"actual_price\"] == 0) and \"selling_price\" in document:\n",
    "        document[\"actual_price\"] = int(int(document[\"selling_price\"])*document[\"discount\"]/100)\n",
    "\n",
    "    # Convert average_rating to float, set to NaN if missing or empty\n",
    "    if \"average_rating\" in document and str(document[\"average_rating\"]).strip() != \"\":\n",
    "        try:\n",
    "            document[\"average_rating\"] = float(document[\"average_rating\"])\n",
    "        except ValueError:\n",
    "            document[\"average_rating\"] = float(\"nan\")\n",
    "    else:\n",
    "        document[\"average_rating\"] = float(\"nan\")\n",
    "\n",
    "    return document\n",
    "\n",
    "def preprocess_document(document):\n",
    "    \"\"\"\n",
    "    Join all preprocessing steps for a document.\n",
    "    \"\"\"\n",
    "\n",
    "    document[\"description\"] = preprocess_text(document[\"description\"])\n",
    "    document[\"title\"] = preprocess_text(document[\"title\"])\n",
    "    document[\"seller\"] = clean_seller(document[\"seller\"])\n",
    "    document[\"brand\"] = document[\"brand\"].lower().split()\n",
    "\n",
    "    document = preprocess_non_textual(document)\n",
    "\n",
    "    return document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "977d4751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFY THIS PATH AS NEEDED\n",
    "file_path = \"../../data/fashion_products_dataset.json\"\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff0e1f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_actual_price(row):\n",
    "    # if actual_price is empty, try to compute it:\n",
    "    # either from selling_price and discount, or just use selling_price\n",
    "    if row['actual_price'] == '':\n",
    "        # Convert selling_price and discount to float if not empty\n",
    "        if row['selling_price'] != '' and row['discount'] != '':\n",
    "            selling_price = float(str(row['selling_price']).replace(',', ''))\n",
    "            discount = float(str(row['discount']).replace('%', '').replace('off', '').strip())\n",
    "            return selling_price * (1 - discount / 100)\n",
    "        elif row['selling_price'] != '':\n",
    "            return float(str(row['selling_price']).replace(',', ''))\n",
    "    return row['actual_price']\n",
    "\n",
    "df['discount'] = df['discount'].replace('', '0')\n",
    "df['actual_price'] = df.apply(impute_actual_price, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50f0e0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the remaining products without price\n",
    "df = df[(df['actual_price'] != '') & (df['selling_price'] != '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ad072e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace empty brand names with 'no brand'\n",
    "df.loc[df['brand'] == '', 'brand'] = 'no brand'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b5f0361",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(preprocess_document, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5f4169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def normalize_cat_token(val):\n",
    "    if pd.isna(val) or str(val).strip() == \"\":\n",
    "        return []\n",
    "    # split common multi-value strings; keep a single value as 1-item list\n",
    "    parts = re.split(r\"[\\/,;|]\", str(val))\n",
    "    return [re.sub(r\"\\s+\", \"_\", p.strip().lower()) for p in parts if p.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90d15d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inverted_index_df(df: pd.DataFrame,id_col: str | None = None,text_cols: list[str] = (\"title\", \"description\", \"product_details\"),min_df: int = 1,store_positions: bool = False):\n",
    "    \"\"\"\n",
    "    df: preprocessed dataframe (title/description/product_details are token lists).\n",
    "    id_col: column holding unique ids; if None, uses df.index (as str).\n",
    "    text_cols: columns with *token lists* (already stemmed, stopwords removed).\n",
    "    min_df: drop terms that appear in < min_df documents.\n",
    "    store_positions: if True, also keep term positions for phrase/proximity queries.\n",
    "    \"\"\"\n",
    "    # assign doc ID's\n",
    "    doc_ids = df[id_col].astype(str).tolist() if id_col else df.index.astype(str).tolist()\n",
    "\n",
    "    per_doc_terms = []\n",
    "    per_doc_sequence = []\n",
    "\n",
    "    # gather tokens for each row\n",
    "    for _, row in df.iterrows():\n",
    "        tokens = []\n",
    "\n",
    "        # text cols are already tokenized lists after preprocess_document(), we just make it robust if something slipped through\n",
    "        for c in text_cols:\n",
    "            if c in df.columns:\n",
    "                vals = row[c]\n",
    "                if isinstance(vals, (list, tuple)):\n",
    "                    tokens.extend([str(t).lower() for t in vals if str(t).strip()])\n",
    "                elif pd.notna(vals):\n",
    "                    # if something slipped through as string, tokenize lightly:\n",
    "                    tokens.extend(re.findall(r\"[A-Za-z0-9]+\", str(vals).lower()))\n",
    "\n",
    "\n",
    "        # ensure we have a sequence for positions and a set for boolean presence\n",
    "        if store_positions:\n",
    "            per_doc_sequence.append(tokens[:])\n",
    "        per_doc_terms.append(set(tokens))\n",
    "\n",
    "    # build postings (term -> list[doc_id]) and df counts\n",
    "    postings_tmp = defaultdict(list)\n",
    "    df_count = defaultdict(int)\n",
    "\n",
    "    for d_i, terms in enumerate(per_doc_terms):\n",
    "        did = doc_ids[d_i]\n",
    "        for term in terms:\n",
    "            postings_tmp[term].append(did)\n",
    "            df_count[term] += 1\n",
    "\n",
    "    # min_df filter + sort postings\n",
    "    postings_tmp = {t: sorted(dids) for t, dids in postings_tmp.items() if df_count[t] >= min_df}\n",
    "\n",
    "    # vocab\n",
    "    vocab = {term: tid for tid, term in enumerate(sorted(postings_tmp.keys()))}\n",
    "    id2term = {tid: term for term, tid in vocab.items()}\n",
    "\n",
    "    # final inverted index (term_id -> [doc_ids])\n",
    "    inv_index = {vocab[t]: dids for t, dids in postings_tmp.items()}\n",
    "\n",
    "    # positional index\n",
    "    positional = None\n",
    "    if store_positions:\n",
    "        positional = {tid: defaultdict(list) for tid in inv_index.keys()}\n",
    "        for d_i, seq in enumerate(per_doc_sequence):\n",
    "            did = doc_ids[d_i]\n",
    "            for pos, tok in enumerate(seq):\n",
    "                if tok in vocab:\n",
    "                    tid = vocab[tok]\n",
    "                    positional[tid][did].append(pos)\n",
    "        # convert inner dicts to normal dicts\n",
    "        positional = {tid: dict(dmap) for tid, dmap in positional.items()}\n",
    "\n",
    "    return {\n",
    "        \"vocab\": vocab,            # term -> term_id\n",
    "        \"id2term\": id2term,        # term_id -> term\n",
    "        \"postings\": inv_index,     # term_id -> [doc_id, ...] (sorted)\n",
    "        \"doc_ids\": doc_ids,        # all doc ids, as strings\n",
    "        \"positional\": positional   # optional: term_id -> {doc_id: [positions]}\n",
    "    }\n",
    "\n",
    "index_obj = build_inverted_index_df(\n",
    "    df,\n",
    "    id_col=None,\n",
    "    text_cols=df.columns,\n",
    "    min_df=1,\n",
    "    store_positions=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddd7f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick lookups\n",
    "def docs_for_term(term: str):\n",
    "    \"\"\"Return document IDs for a raw term or categorical token (e.g., 'brand:nike').\"\"\"\n",
    "    tid = index_obj[\"vocab\"].get(term)\n",
    "    return index_obj[\"postings\"].get(tid, []) if tid is not None else []\n",
    "\n",
    "def doc_positions_for_term(term: str, doc_id: str):\n",
    "    \"\"\"Return positions of term in a specific document.\"\"\"\n",
    "    tid = index_obj[\"vocab\"].get(term)\n",
    "    if tid is None:\n",
    "        return []\n",
    "    return index_obj[\"positional\"].get(tid, {}).get(doc_id, [])\n",
    "\n",
    "def and_query(terms: list[str]):\n",
    "    \"\"\"Boolean AND over terms.\"\"\"\n",
    "    sets = [set(docs_for_term(t)) for t in terms]\n",
    "    return sorted(set.intersection(*sets)) if sets else []\n",
    "\n",
    "def or_query(terms: list[str]):\n",
    "    \"\"\"Boolean OR over terms.\"\"\"\n",
    "    s = set()\n",
    "    for t in terms:\n",
    "        s.update(docs_for_term(t))\n",
    "    return sorted(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b31c3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = {\n",
    "    \"Q1\": \"cotton tshirt 50 100 men blue\",\n",
    "    \"Q2\": \"adidas red\",\n",
    "    \"Q3\": \"denim jean skinny\",\n",
    "    \"Q4\": \"dress red\",\n",
    "    \"Q5\": \"leather jacket\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb0d2371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tf(word, document):\n",
    "    \"\"\"\n",
    "    Calculate term frequency for a word in a document.\n",
    "    TF = Number of times term t appears in a document\n",
    "    \"\"\"\n",
    "    return document.count(word)    \n",
    "    \n",
    "\n",
    "def calculate_idf(word, all_documents):\n",
    "    \"\"\"\n",
    "    Calculate inverse document frequency for a word.\n",
    "    IDF = log(Total number of documents / Number of documents containing term t)\n",
    "    \"\"\"\n",
    "    num_documents_with_term = len(docs_for_term(word))\n",
    "    if num_documents_with_term == 0:\n",
    "        return 0\n",
    "    return np.log(len(all_documents) / num_documents_with_term)\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors.\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
    "        return 0\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62a2ab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents(query, documents, k):\n",
    "    \"\"\"\n",
    "    Rank documents based on TF-IDF scores for the given query.\n",
    "    Return the top k documents.\n",
    "    \"\"\"\n",
    "    all_documents = [doc[\"description\"] + doc[\"title\"] + doc[\"brand\"] for index, doc in documents.iterrows()]\n",
    "    scores = []\n",
    "\n",
    "    term_idfs = {term: calculate_idf(term, all_documents) for term in query}\n",
    "    query_vector = np.array([calculate_tf(term, query) * term_idfs[term] for term in query])\n",
    "\n",
    "    for index, doc in documents.iterrows():\n",
    "        doc_vec = []\n",
    "        doc_text = doc[\"description\"] + doc[\"title\"] + doc[\"brand\"]\n",
    "        for term in query:\n",
    "            tf = calculate_tf(term, doc_text)\n",
    "            if tf > 0:\n",
    "                # used the formula tf = 1 + log_10(count)\n",
    "                doc_vec.append((1 + np.log(tf)) * term_idfs[term])\n",
    "            else:\n",
    "                doc_vec.append(0)\n",
    "        scores.append((doc, cosine_similarity(query_vector, np.array(doc_vec))))\n",
    "\n",
    "    # Sort documents by score in descending order\n",
    "    ranked_docs = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return ranked_docs[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08af6677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(retrieved_docs, relevant_docs, k):\n",
    "    \"\"\"\n",
    "    Calculate Precision@k.\n",
    "    Precision@k = (Number of relevant documents retrieved in top k) / k\n",
    "    \"\"\"\n",
    "    retrieved_at_k = retrieved_docs[:k]\n",
    "    relevant_retrieved = sum(1 for doc in retrieved_at_k if doc[\"pid\"] in relevant_docs)\n",
    "    return relevant_retrieved / k if k > 0 else 0\n",
    "\n",
    "def recall_at_k(retrieved_docs, relevant_docs, k):\n",
    "    \"\"\"\n",
    "    Calculate Recall@k.\n",
    "    Recall@k = (Number of relevant documents retrieved in top k) / (Total number of relevant documents)\n",
    "    \"\"\"\n",
    "    retrieved_at_k = retrieved_docs[:k]\n",
    "    relevant_retrieved = sum(1 for doc in retrieved_at_k if doc[\"pid\"] in relevant_docs)\n",
    "    total_relevant = len(relevant_docs)\n",
    "    return relevant_retrieved / total_relevant if total_relevant > 0 else 0\n",
    "\n",
    "def average_precision_at_k(retrieved_docs, relevant_docs, k):\n",
    "    \"\"\"\n",
    "    Calculate Average Precision@k.\n",
    "    AP@k = Average of Precision@i for each relevant document retrieved in top k\n",
    "    \"\"\"\n",
    "    retrieved_at_k = retrieved_docs[:k]\n",
    "    relevant_retrieved = 0\n",
    "    precision_sum = 0\n",
    "\n",
    "    for i, doc in enumerate(retrieved_at_k, start=1):\n",
    "        if doc[\"pid\"] in relevant_docs:\n",
    "            relevant_retrieved += 1\n",
    "            precision_sum += relevant_retrieved / i\n",
    "\n",
    "    return precision_sum / relevant_retrieved if relevant_retrieved > 0 else 0\n",
    "\n",
    "def f1_score(precision, recall):\n",
    "    \"\"\"\n",
    "    Calculate F1 Score.\n",
    "    F1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "    \"\"\"\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def f1_score_at_k(retrieved_docs, relevant_docs, k):\n",
    "    \"\"\"\n",
    "    Calculate F1 Score at k.\n",
    "    \"\"\"\n",
    "    precision = precision_at_k(retrieved_docs, relevant_docs, k)\n",
    "    recall = recall_at_k(retrieved_docs, relevant_docs, k)\n",
    "    return f1_score(precision, recall)\n",
    "\n",
    "def mean_average_precision(retrieved_docs_list, relevant_docs_list, k):\n",
    "    \"\"\"\n",
    "    Calculate Mean Average Precision (MAP) at k.\n",
    "    MAP = Mean of Average Precision@k over all queries\n",
    "    \"\"\"\n",
    "    ap_sum = 0\n",
    "    num_queries = len(retrieved_docs_list)\n",
    "\n",
    "    for retrieved_docs, relevant_docs in zip(retrieved_docs_list, relevant_docs_list):\n",
    "        ap_sum += average_precision_at_k(retrieved_docs, relevant_docs, k)\n",
    "\n",
    "    return ap_sum / num_queries if num_queries > 0 else 0\n",
    "\n",
    "def reciprocal_rank(retrieved_docs, relevant_docs):\n",
    "    \"\"\"\n",
    "    Calculate Reciprocal Rank (RR).\n",
    "    RR = 1 / Rank of the first relevant document\n",
    "    \"\"\"\n",
    "    rank = 0\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        if doc[\"pid\"] in relevant_docs:\n",
    "            rank = i + 1\n",
    "            break\n",
    "    return 1 / rank if rank > 0 else 0\n",
    "\n",
    "def mean_reciprocal_rank(retrieved_docs_list, relevant_docs_list):\n",
    "    \"\"\"\n",
    "    Calculate Mean Reciprocal Rank (MRR).\n",
    "    MRR = Mean of Reciprocal Ranks over all queries\n",
    "    \"\"\"\n",
    "    rr_sum = 0\n",
    "    num_queries = len(retrieved_docs_list)\n",
    "\n",
    "    for retrieved_docs, relevant_docs in zip(retrieved_docs_list, relevant_docs_list):\n",
    "        rr_sum += reciprocal_rank(retrieved_docs, relevant_docs)\n",
    "\n",
    "    return rr_sum / num_queries if num_queries > 0 else 0\n",
    "\n",
    "def dcg_at_k(retrieved_docs, relevant_docs, k):\n",
    "    \"\"\"\n",
    "    Calculate Discounted Cumulative Gain (DCG) at k.\n",
    "    DCG@k = Sum of (relevance of document at rank i) / log2(i + 1) for i in 1 to k\n",
    "    \"\"\"\n",
    "    dcg = 0\n",
    "    for i in range(min(k, len(retrieved_docs))):\n",
    "        doc = retrieved_docs[i]\n",
    "        if doc[\"pid\"] in relevant_docs:\n",
    "            relevance = 1  # we only have binary relevance\n",
    "        else:\n",
    "            relevance = 0\n",
    "        dcg += relevance / np.log2(i + 2)  # i + 2 because i starts from 0\n",
    "    return dcg\n",
    "\n",
    "def ndcg_at_k(retrieved_docs, relevant_docs, k):\n",
    "    \"\"\"\n",
    "    Calculate Normalized Discounted Cumulative Gain (NDCG) at k.\n",
    "    NDCG@k = DCG@k / IDCG@k\n",
    "    \"\"\"\n",
    "    dcg = dcg_at_k(retrieved_docs, relevant_docs, k)\n",
    "    \n",
    "    ideal_retrieved_docs = [{\"pid\": pid} for pid in relevant_docs]\n",
    "    idcg = dcg_at_k(ideal_retrieved_docs, relevant_docs, k)\n",
    "    \n",
    "    return dcg / idcg if idcg > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3_prep_globals",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a text-only view per document (description + title + brand)\n",
    "ALL_TEXT_DOCS = [row[\"description\"] + row[\"title\"] + row[\"brand\"] for _, row in df.iterrows()]\n",
    "N_TEXT = len(ALL_TEXT_DOCS)\n",
    "\n",
    "# Document lengths and average length\n",
    "DOC_LENGTHS = np.array([len(toks) for toks in ALL_TEXT_DOCS], dtype=float)\n",
    "AVG_DL = float(DOC_LENGTHS.mean()) if N_TEXT > 0 else 0.0\n",
    "\n",
    "# Document frequency per term over the text only view\n",
    "from collections import Counter\n",
    "DF_TEXT = Counter()\n",
    "for toks in ALL_TEXT_DOCS:\n",
    "    DF_TEXT.update(set(toks))\n",
    "\n",
    "def idf_text(term: str) -> float:\n",
    "    \"\"\"IDF using text only df: log(N / df)\"\"\"\n",
    "    df_t = DF_TEXT.get(term, 0)\n",
    "    if df_t == 0 or N_TEXT == 0:\n",
    "        return 0.0\n",
    "    return np.log(N_TEXT / df_t)\n",
    "\n",
    "def idf_bm25(term: str) -> float:\n",
    "    \"\"\"BM25 IDF: log((N - df + 0.5) / (df + 0.5) + 1)\"\"\"\n",
    "    df_t = DF_TEXT.get(term, 0)\n",
    "    return np.log(((N_TEXT - df_t + 0.5) / (df_t + 0.5)) + 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3_pipeline_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(q):\n",
    "    \"\"\"Accepts a string or list of strings; returns normalized tokens using the same preprocessing as documents\"\"\"\n",
    "    if isinstance(q, str):\n",
    "        return preprocess_text(q)\n",
    "    elif isinstance(q, (list, tuple)):\n",
    "        return preprocess_text(\" \".join(map(str, q)))\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def retrieve_conjunctive_candidates(query_tokens):\n",
    "    \"\"\"Return candidate doc indices (list of ints) where all query terms appear (AND)\"\"\"\n",
    "    if not query_tokens:\n",
    "        return []\n",
    "    dids = and_query(query_tokens)\n",
    "    return [int(d) for d in dids]\n",
    "\n",
    "def doc_text_tokens(row):\n",
    "    return row[\"description\"] + row[\"title\"] + row[\"brand\"]\n",
    "\n",
    "def cosine(v1, v2):\n",
    "    return cosine_similarity(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3_rankers_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_tfidf_cosine(query_tokens, candidate_indices, k=10):\n",
    "    # Unique terms to define vector space\n",
    "    terms = list(dict.fromkeys(query_tokens))\n",
    "    # Query vector (tf-idf)\n",
    "    q_vec = np.array([(1 + np.log(query_tokens.count(t))) * idf_text(t) if query_tokens.count(t) > 0 else 0.0 for t in terms])\n",
    "\n",
    "    scored = []\n",
    "    for idx in candidate_indices:\n",
    "        row = df.iloc[idx]\n",
    "        toks = doc_text_tokens(row)\n",
    "        d_vec = []\n",
    "        for t in terms:\n",
    "            tf = toks.count(t)\n",
    "            d_vec.append(((1 + np.log(tf)) * idf_text(t)) if tf > 0 else 0.0)\n",
    "        d_vec = np.array(d_vec)\n",
    "        score = cosine(q_vec, d_vec)\n",
    "        scored.append((row, score))\n",
    "    return sorted(scored, key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "def rank_bm25(query_tokens, candidate_indices, k=10, k1=1.5, b=0.75):\n",
    "    scored = []\n",
    "    for idx in candidate_indices:\n",
    "        row = df.iloc[idx]\n",
    "        toks = doc_text_tokens(row)\n",
    "        dl = len(toks)\n",
    "        score = 0.0\n",
    "        for t in set(query_tokens):\n",
    "            tf = toks.count(t)\n",
    "            if tf == 0:\n",
    "                continue\n",
    "            idf = idf_bm25(t)\n",
    "            denom = tf + k1 * (1 - b + b * (dl / (AVG_DL if AVG_DL > 0 else 1.0)))\n",
    "            score += idf * (tf * (k1 + 1)) / denom\n",
    "        scored.append((row, score))\n",
    "    return sorted(scored, key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "# Global numeric ranges for normalization\n",
    "SELL_MIN, SELL_MAX = float(pd.to_numeric(df['selling_price'], errors='coerce').min()), float(pd.to_numeric(df['selling_price'], errors='coerce').max())\n",
    "DISC_MIN, DISC_MAX = float(pd.to_numeric(df['discount'], errors='coerce').min()), float(pd.to_numeric(df['discount'], errors='coerce').max())\n",
    "RAT_MIN, RAT_MAX = 0.0, 5.0  # ratings are on 0..5 scale\n",
    "\n",
    "def _norm(x, lo, hi):\n",
    "    try:\n",
    "        xv = float(x)\n",
    "    except Exception:\n",
    "        xv = 0.0\n",
    "    if hi <= lo:\n",
    "        return 0.0\n",
    "    return (xv - lo) / (hi - lo)\n",
    "\n",
    "def rank_custom_hybrid(query_tokens, candidate_indices, k=10,\n",
    "                       base='bm25', k1=1.5, b=0.75,\n",
    "                       w_rating=0.30, w_discount=0.20, w_price=0.10):\n",
    "    \"\"\"\n",
    "    Custom score = BaseTextScore * (1 + w_rating*rating_norm + w_discount*discount_norm - w_price*price_norm)\n",
    "    - BaseTextScore: BM25 (default) or TF‑IDF cosine\n",
    "    - Boosts: higher rating and discount help; higher price penalizes slightly\n",
    "    \"\"\"\n",
    "    # Precompute base scores once\n",
    "    if base == 'bm25':\n",
    "        base_scored = rank_bm25(query_tokens, candidate_indices, k=len(candidate_indices), k1=k1, b=b)\n",
    "    else:\n",
    "        base_scored = rank_tfidf_cosine(query_tokens, candidate_indices, k=len(candidate_indices))\n",
    "\n",
    "    out = []\n",
    "    for row, base_score in base_scored:\n",
    "        rating = 0.0 if pd.isna(row.get('average_rating', np.nan)) else float(row['average_rating'])\n",
    "        discount = row.get('discount', 0)\n",
    "        price = row.get('selling_price', 0)\n",
    "\n",
    "        rating_n = _norm(rating, RAT_MIN, RAT_MAX)\n",
    "        discount_n = _norm(discount, DISC_MIN, DISC_MAX)\n",
    "        price_n = _norm(price, SELL_MIN, SELL_MAX)\n",
    "\n",
    "        factor = 1.0 + (w_rating * rating_n) + (w_discount * discount_n) - (w_price * price_n)\n",
    "        final_score = float(base_score) * max(factor, 0.0)\n",
    "        out.append((row, final_score))\n",
    "\n",
    "    return sorted(out, key=lambda x: x[1], reverse=True)[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3_demo_rank_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_pid_title(scored, k=10):\n",
    "    out = []\n",
    "    for row, s in scored[:k]:\n",
    "        title_str = \" \".join(row['title']) if isinstance(row['title'], list) else str(row['title'])\n",
    "        out.append((row['pid'], round(float(s), 4), title_str))\n",
    "    return out\n",
    "\n",
    "demo_results = {}\n",
    "for qid, q_terms in test_queries.items():\n",
    "    q_tokens = preprocess_query(q_terms)\n",
    "    cand_idx = retrieve_conjunctive_candidates(q_tokens)\n",
    "    tfidf_res = rank_tfidf_cosine(q_tokens, cand_idx, k=10)\n",
    "    bm25_res = rank_bm25(q_tokens, cand_idx, k=10)\n",
    "    custom_res = rank_custom_hybrid(q_tokens, cand_idx, k=10, base='bm25')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3_w2v_train",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "w2v_dim = 100\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=ALL_TEXT_DOCS,\n",
    "    vector_size=w2v_dim,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    sg=1,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3_w2v_helpers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_w2v(tokens, model):\n",
    "    vecs = [model.wv[t] for t in tokens if t in model.wv]\n",
    "    if not vecs:\n",
    "        return None\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "def rank_w2v_cosine(query_tokens, candidate_indices, k=20):\n",
    "    q_vec = average_w2v(query_tokens, w2v_model)\n",
    "    if q_vec is None:\n",
    "        return []\n",
    "    scored = []\n",
    "    for idx in candidate_indices:\n",
    "        row = df.iloc[idx]\n",
    "        d_vec = average_w2v(doc_text_tokens(row), w2v_model)\n",
    "        if d_vec is None:\n",
    "            continue\n",
    "        score = cosine(q_vec, d_vec)\n",
    "        scored.append((row, score))\n",
    "    return sorted(scored, key=lambda x: x[1], reverse=True)[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3_w2v_run",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_results = {}\n",
    "for qid, q_terms in test_queries.items():\n",
    "    q_tokens = preprocess_query(q_terms)\n",
    "    cand_idx = retrieve_conjunctive_candidates(q_tokens)\n",
    "    w2v_scored = rank_w2v_cosine(q_tokens, cand_idx, k=20)\n",
    "    w2v_results[qid] = [(row['pid'], round(float(score), 4), \" \".join(row['title']) if isinstance(row['title'], list) else str(row['title'])) for row, score in w2v_scored]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c25dda4",
   "metadata": {},
   "source": [
    "## RAG & Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f440770-a824-4775-aca1-509227910363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_PATH = \"../../data/fashion_products_dataset.json\"\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de237f0e-35ad-4381-beca-9ca72f2d58e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Engine Initialized.\n"
     ]
    }
   ],
   "source": [
    "class SearchEngine:\n",
    "    def __init__(self, file_path):\n",
    "        self.df = self.load_and_clean_data(file_path)\n",
    "        self.index = self.build_index()\n",
    "        self.doc_lengths = self.compute_doc_lengths()\n",
    "        self.avg_dl = float(np.mean(self.doc_lengths))\n",
    "        self.N = len(self.df)\n",
    "        print(\"Search Engine Initialized.\")\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        if not isinstance(text, str): return []\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [t.lower() for t in tokens]\n",
    "        tokens = [re.sub(r\"[^\\w\\s]\", \"\", t) for t in tokens]\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [t for t in tokens if t not in stop_words and t]\n",
    "        stemmer = PorterStemmer()\n",
    "        return [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "    def load_and_clean_data(self, path):\n",
    "        with open(path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        def clean_row(row):\n",
    "            # 1. Prices\n",
    "            try:\n",
    "                sp = float(str(row['selling_price']).replace(',','')) if row['selling_price'] else 0.0\n",
    "            except: sp = 0.0\n",
    "            \n",
    "            disc = 0\n",
    "            if isinstance(row['discount'], str) and '%' in row['discount']:\n",
    "                 disc = int(re.search(r'\\d+', row['discount']).group())\n",
    "            \n",
    "            ap = row['actual_price']\n",
    "            if not ap:\n",
    "                ap = sp * (1 + disc/100) if disc > 0 else sp\n",
    "            else:\n",
    "                try: ap = float(str(ap).replace(',',''))\n",
    "                except: ap = sp\n",
    "\n",
    "            # 2. Text\n",
    "            row['processed_title'] = self.preprocess_text(row['title'])\n",
    "            row['processed_desc'] = self.preprocess_text(row['description'])\n",
    "            brand = row['brand'] if row['brand'] else \"no brand\"\n",
    "            row['processed_brand'] = [brand.lower()]\n",
    "            \n",
    "            # Combine for indexing\n",
    "            row['all_tokens'] = row['processed_title'] + row['processed_desc'] + row['processed_brand']\n",
    "            \n",
    "            # 3. Numeric Types for sorting\n",
    "            row['selling_price_val'] = sp\n",
    "            row['actual_price_val'] = ap\n",
    "            row['discount_val'] = disc\n",
    "            try: row['rating_val'] = float(row['average_rating'])\n",
    "            except: row['rating_val'] = 0.0\n",
    "            \n",
    "            return row\n",
    "\n",
    "        df = df.apply(clean_row, axis=1)\n",
    "        # Generate a unique string ID for URLs if pid is missing or complex\n",
    "        df['uid'] = [str(uuid.uuid4()) for _ in range(len(df))] \n",
    "        return df\n",
    "\n",
    "    def build_index(self):\n",
    "        inv_index = defaultdict(list)\n",
    "        for idx, row in self.df.iterrows():\n",
    "            # Use set for boolean retrieval to avoid duplicates per doc\n",
    "            for term in set(row['all_tokens']):\n",
    "                inv_index[term].append(idx)\n",
    "        return inv_index\n",
    "\n",
    "    def compute_doc_lengths(self):\n",
    "        return np.array([len(tokens) for tokens in self.df['all_tokens']])\n",
    "\n",
    "    def get_idf(self, term):\n",
    "        df_count = len(self.index.get(term, []))\n",
    "        return np.log(1 + (self.N - df_count + 0.5) / (df_count + 0.5))\n",
    "\n",
    "    def search(self, query, k=20):\n",
    "        \"\"\"\n",
    "        Implements BM25 + Boosting (Your 'Custom' logic).\n",
    "        \"\"\"\n",
    "        q_tokens = self.preprocess_text(query)\n",
    "        \n",
    "        # 1. Retrieve (Conjunctive OR to ensure recall, ranked by BM25)\n",
    "        doc_scores = defaultdict(float)\n",
    "        k1, b = 1.5, 0.75\n",
    "        \n",
    "        relevant_indices = set()\n",
    "        for t in q_tokens:\n",
    "            relevant_indices.update(self.index.get(t, []))\n",
    "            \n",
    "        if not relevant_indices:\n",
    "            return []\n",
    "\n",
    "        for idx in relevant_indices:\n",
    "            doc_tokens = self.df.iloc[idx]['all_tokens']\n",
    "            dl = len(doc_tokens)\n",
    "            score = 0\n",
    "            for t in q_tokens:\n",
    "                tf = doc_tokens.count(t) # Note: In prod, pre-calculate TF\n",
    "                if tf > 0:\n",
    "                    idf = self.get_idf(t)\n",
    "                    num = tf * (k1 + 1)\n",
    "                    den = tf + k1 * (1 - b + b * (dl / self.avg_dl))\n",
    "                    score += idf * (num / den)\n",
    "            \n",
    "            # 2. Custom Boosting (Rating & Discount)\n",
    "            row = self.df.iloc[idx]\n",
    "            boost = 1.0\n",
    "            if row['rating_val'] > 4.0: boost += 0.2\n",
    "            if row['discount_val'] > 30: boost += 0.1\n",
    "            \n",
    "            doc_scores[idx] = score * boost\n",
    "\n",
    "        sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "        \n",
    "        results = []\n",
    "        for idx, score in sorted_docs:\n",
    "            results.append(self.df.iloc[idx].to_dict())\n",
    "            results[-1]['search_score'] = score\n",
    "            \n",
    "        return results\n",
    "\n",
    "# Initialize Engine\n",
    "engine = SearchEngine(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f206e8c1-f412-469d-9d1f-4c1e0adbf64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "class RAGSystem:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        self.client = None\n",
    "        \n",
    "        try:\n",
    "            self.client = Groq(api_key=api_key)\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing Groq Client: {e}\")\n",
    "\n",
    "    def generate_summary(self, query, top_results):\n",
    "        \"\"\"\n",
    "        Improvement 1: Context Pruning. Only take top 5 results.\n",
    "        Improvement 2: Metadata Injection. Include price/rating in context.\n",
    "        \"\"\"\n",
    "        if not top_results:\n",
    "            return \"No products found to summarize.\"\n",
    "\n",
    "        context_text = \"\"\n",
    "        for i, res in enumerate(top_results[:5]): # Only top 5\n",
    "            context_text += f\"Item {i+1}: {res['title']}. Price: {res['selling_price']}. Rating: {res['average_rating']}. Description: {res['description'][:200]}...\\n\"\n",
    "\n",
    "        # Improvement 3: Refined Prompt\n",
    "        prompt = f\"\"\"\n",
    "        You are an expert Fashion Shopping Assistant. \n",
    "        User Query: \"{query}\"\n",
    "        \n",
    "        Based ONLY on the following available products, provide a 3-sentence summary helping the user choose. \n",
    "        Highlight the best value (high rating + low price) if it exists.\n",
    "        \n",
    "        Products:\n",
    "        {context_text}\n",
    "        \n",
    "        Summary:\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            if self.api_key == \"YOUR_GROQ_API_KEY\":\n",
    "                return \"LLM Summary Placeholder: Please insert a valid Groq API Key.\"\n",
    "            \n",
    "            if self.client is None:\n",
    "                return \"Error: Groq Client not initialized. Check your API Key.\"\n",
    "\n",
    "            # 3. Call the Groq API\n",
    "            # Note: The syntax remains nearly identical to OpenAI\n",
    "            response = self.client.chat.completions.create(\n",
    "                # 4. Use a Groq-supported model (e.g., Llama 3 or Mixtral)\n",
    "                model=\"moonshotai/kimi-k2-instruct-0905\", \n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=150\n",
    "            )\n",
    "            \n",
    "            # Access response attributes (Standard OpenAI format)\n",
    "            return response.choices[0].message.content.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generating summary: {str(e)}\"\n",
    "\n",
    "rag = RAGSystem(GROQ_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a6c6695-08e6-4a24-8ba1-c981d0ece729",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalyticsStore:\n",
    "    def __init__(self):\n",
    "        # 1. Sessions (User Context)\n",
    "        self.sessions = [] # [{'session_id', 'user_agent', 'timestamp', 'ip'}]\n",
    "        \n",
    "        # 2. Requests (Search Queries)\n",
    "        self.searches = [] # [{'search_id', 'session_id', 'query', 'timestamp', 'num_results'}]\n",
    "        \n",
    "        # 3. Clicks (Interaction)\n",
    "        self.clicks = []   # [{'session_id', 'search_id', 'doc_uid', 'rank', 'timestamp', 'dwell_time'}]\n",
    "\n",
    "    def track_session(self, user_agent, ip):\n",
    "        sid = str(uuid.uuid4())\n",
    "        self.sessions.append({\n",
    "            'session_id': sid,\n",
    "            'user_agent': user_agent,\n",
    "            'ip': ip,\n",
    "            'timestamp': datetime.datetime.now().isoformat()\n",
    "        })\n",
    "        return sid\n",
    "\n",
    "    def track_search(self, session_id, query, num_results):\n",
    "        search_id = str(uuid.uuid4())\n",
    "        self.searches.append({\n",
    "            'search_id': search_id,\n",
    "            'session_id': session_id,\n",
    "            'query': query,\n",
    "            'num_results': num_results,\n",
    "            'timestamp': datetime.datetime.now().isoformat()\n",
    "        })\n",
    "        return search_id\n",
    "\n",
    "    def track_click(self, session_id, search_id, doc_uid, rank):\n",
    "        self.clicks.append({\n",
    "            'session_id': session_id,\n",
    "            'search_id': search_id,\n",
    "            'doc_uid': doc_uid,\n",
    "            'rank': rank,\n",
    "            'timestamp': datetime.datetime.now().isoformat(),\n",
    "            'dwell_time': 0 # Updated later via Beacon\n",
    "        })\n",
    "\n",
    "    def update_dwell_time(self, session_id, doc_uid, duration):\n",
    "        # Find the most recent click for this session/doc and update\n",
    "        for click in reversed(self.clicks):\n",
    "            if click['session_id'] == session_id and click['doc_uid'] == doc_uid:\n",
    "                click['dwell_time'] = duration\n",
    "                break\n",
    "\n",
    "    def get_stats(self):\n",
    "        total_searches = len(self.searches)\n",
    "        total_clicks = len(self.clicks)\n",
    "        \n",
    "        # Top Queries\n",
    "        queries = [s['query'] for s in self.searches]\n",
    "        top_queries = Counter(queries).most_common(5)\n",
    "        \n",
    "        # Click Through Rate (CTR)\n",
    "        ctr = (total_clicks / total_searches * 100) if total_searches > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'total_searches': total_searches,\n",
    "            'total_clicks': total_clicks,\n",
    "            'ctr': round(ctr, 2),\n",
    "            'top_queries': top_queries,\n",
    "            'raw_sessions': self.sessions[-5:], # Last 5\n",
    "            'raw_clicks': self.clicks[-5:]\n",
    "        }\n",
    "\n",
    "analytics = AnalyticsStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "911639cf-3585-4ee7-9a6d-e1b931b5846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CSS STYLES ---\n",
    "CSS = \"\"\"\n",
    "<style>\n",
    "    body { font-family: 'Segoe UI', sans-serif; max-width: 1000px; margin: 0 auto; padding: 20px; background-color: #f9f9f9; }\n",
    "    .header { text-align: center; margin-bottom: 40px; }\n",
    "    .search-box { display: flex; justify-content: center; gap: 10px; margin-bottom: 30px; }\n",
    "    input[type=\"text\"] { width: 60%; padding: 12px; border: 1px solid #ddd; border-radius: 4px; font-size: 16px; }\n",
    "    button { padding: 12px 24px; background-color: #333; color: white; border: none; border-radius: 4px; cursor: pointer; }\n",
    "    button:hover { background-color: #555; }\n",
    "    \n",
    "    .rag-box { background: #e8f4fd; border: 1px solid #b6dbf9; padding: 15px; border-radius: 8px; margin-bottom: 20px; }\n",
    "    .rag-title { font-weight: bold; color: #0056b3; margin-bottom: 5px; }\n",
    "    \n",
    "    .result-item { background: white; padding: 20px; margin-bottom: 15px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.05); }\n",
    "    .result-title a { font-size: 18px; color: #333; text-decoration: none; font-weight: bold; }\n",
    "    .result-meta { color: #666; font-size: 14px; margin: 5px 0; }\n",
    "    .price { color: #d9534f; font-weight: bold; }\n",
    "    \n",
    "    .dashboard-grid { display: grid; grid-template-columns: 1fr 1fr; gap: 20px; }\n",
    "    .card { background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }\n",
    "    table { width: 100%; border-collapse: collapse; }\n",
    "    th, td { padding: 8px; text-align: left; border-bottom: 1px solid #ddd; }\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "# --- TEMPLATES ---\n",
    "\n",
    "HOME_TEMPLATE = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head><title>Fashion Search</title>\"\"\" + CSS + \"\"\"</head>\n",
    "<body>\n",
    "    <div class=\"header\">\n",
    "        <h1>Fashion Search Engine</h1>\n",
    "        <p>Find the best clothes with AI</p>\n",
    "    </div>\n",
    "    <form action=\"/search\" method=\"get\" class=\"search-box\">\n",
    "        <input type=\"text\" name=\"q\" placeholder=\"e.g. Red cotton summer dress\" required>\n",
    "        <button type=\"submit\">Search</button>\n",
    "    </form>\n",
    "    <div style=\"text-align:center;\">\n",
    "        <a href=\"/dashboard\">View Analytics Dashboard</a>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "RESULTS_TEMPLATE = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head><title>Results for {{ query }}</title>\"\"\" + CSS + \"\"\"</head>\n",
    "<body>\n",
    "    <div class=\"header\">\n",
    "        <form action=\"/search\" method=\"get\" class=\"search-box\">\n",
    "            <input type=\"text\" name=\"q\" value=\"{{ query }}\">\n",
    "            <button type=\"submit\">Search</button>\n",
    "        </form>\n",
    "    </div>\n",
    "\n",
    "    <div class=\"rag-box\">\n",
    "        <div class=\"rag-title\">AI Summary</div>\n",
    "        <div>{{ summary }}</div>\n",
    "    </div>\n",
    "\n",
    "    {% for doc in results %}\n",
    "    <div class=\"result-item\">\n",
    "        <div class=\"result-title\">\n",
    "            <a href=\"/product/{{ doc.uid }}?sid={{ session_id }}&qid={{ search_id }}&rank={{ loop.index }}\" \n",
    "               onclick=\"trackClick('{{ session_id }}', '{{ search_id }}', '{{ doc.uid }}', {{ loop.index }})\">\n",
    "               {{ doc.title }}\n",
    "            </a>\n",
    "        </div>\n",
    "        <div class=\"result-meta\">\n",
    "            <span class=\"price\">{{ doc.selling_price }}</span> \n",
    "            {% if doc.discount_val > 0 %} <span style=\"color:green\">({{ doc.discount }} off)</span> {% endif %}\n",
    "            | Rating: {{ doc.average_rating }} ★ | Brand: {{ doc.brand }}\n",
    "        </div>\n",
    "        <div style=\"color:#555;\">{{ doc.description[:150] }}...</div>\n",
    "    </div>\n",
    "    {% endfor %}\n",
    "    \n",
    "    <script>\n",
    "    function trackClick(sid, qid, docId, rank) {\n",
    "        // We use the href for navigation, but we could fire an async fetch here if we prevented default\n",
    "        // The simple link with query params handles the tracking on the server side /product route\n",
    "    }\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "DETAILS_TEMPLATE = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head><title>{{ doc.title }}</title>\"\"\" + CSS + \"\"\"</head>\n",
    "<body>\n",
    "    <a href=\"javascript:history.back()\">← Back to results</a>\n",
    "    \n",
    "    <div class=\"card\" style=\"margin-top:20px;\">\n",
    "        <h1>{{ doc.title }}</h1>\n",
    "        <h2 class=\"price\">{{ doc.selling_price }}</h2>\n",
    "        <p><strong>Brand:</strong> {{ doc.brand }}</p>\n",
    "        <p><strong>Rating:</strong> {{ doc.average_rating }} / 5.0</p>\n",
    "        <hr>\n",
    "        <h3>Description</h3>\n",
    "        <p>{{ doc.description }}</p>\n",
    "        <br>\n",
    "        <h3>Product Details</h3>\n",
    "        <p>{{ doc.product_details }}</p>\n",
    "        <br>\n",
    "        <a href=\"{{ doc.url }}\" target=\"_blank\" style=\"background:black; color:white; padding:10px 20px; text-decoration:none; border-radius:4px;\">Buy on Original Site</a>\n",
    "    </div>\n",
    "\n",
    "    <script>\n",
    "        // WEB ANALYTICS: Dwell Time Tracking\n",
    "        let startTime = Date.now();\n",
    "        let sid = \"{{ session_id }}\";\n",
    "        let docUid = \"{{ doc.uid }}\";\n",
    "\n",
    "        window.addEventListener(\"beforeunload\", function() {\n",
    "            let endTime = Date.now();\n",
    "            let duration = (endTime - startTime) / 1000; // seconds\n",
    "            \n",
    "            // Send beacon (reliable on page unload)\n",
    "            navigator.sendBeacon(\"/track_dwell\", JSON.stringify({\n",
    "                session_id: sid,\n",
    "                doc_uid: docUid,\n",
    "                duration: duration\n",
    "            }));\n",
    "        });\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "DASHBOARD_TEMPLATE = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Analytics Dashboard</title>\n",
    "    \"\"\" + CSS + \"\"\"\n",
    "    <script src=\"https://cdn.jsdelivr.net/npm/chart.js\"></script>\n",
    "    <style>\n",
    "        .chart-container {\n",
    "            position: relative; \n",
    "            height: 300px; \n",
    "            width: 100%; \n",
    "            margin-bottom: 20px;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Web Analytics Dashboard</h1>\n",
    "    <a href=\"/\">← Back to Search</a>\n",
    "    <br><br>\n",
    "\n",
    "    <div class=\"dashboard-grid\">\n",
    "        <div class=\"card\">\n",
    "            <h3>Key Metrics</h3>\n",
    "            <p><strong>Total Searches:</strong> {{ stats.total_searches }}</p>\n",
    "            <p><strong>Total Clicks:</strong> {{ stats.total_clicks }}</p>\n",
    "            <p><strong>CTR:</strong> {{ stats.ctr }}%</p>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"card\">\n",
    "            <h3>Popular Trends</h3>\n",
    "            <div class=\"chart-container\">\n",
    "                <canvas id=\"queryChart\"></canvas>\n",
    "            </div>\n",
    "            <p style=\"font-size: 0.9em; color: #666;\">\n",
    "                *Visualizes the top 5 most frequent search terms.\n",
    "            </p>\n",
    "        </div>\n",
    "    </div>\n",
    "    \n",
    "    <br>\n",
    "\n",
    "    <div class=\"card\">\n",
    "        <h3>User Activity Timeline</h3>\n",
    "        <p style=\"font-size: 0.9em; color: #666;\">\n",
    "            Comparing search volume vs. product clicks over time (per minute).\n",
    "        </p>\n",
    "        <div class=\"chart-container\">\n",
    "            <canvas id=\"timeChart\"></canvas>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <br>\n",
    "    \n",
    "    <div class=\"card\">\n",
    "        <h3>Recent Interactions (Log)</h3>\n",
    "        <table>\n",
    "            <tr><th>Time</th><th>Type</th><th>Detail</th></tr>\n",
    "            {% for c in stats.raw_clicks|reverse %}\n",
    "            <tr>\n",
    "                <td>{{ c.timestamp }}</td>\n",
    "                <td>CLICK</td>\n",
    "                <td>Rank {{ c.rank }} (Dwell: {{ c.dwell_time|round(1) }}s)</td>\n",
    "            </tr>\n",
    "            {% endfor %}\n",
    "            {% for s in stats.raw_sessions|reverse %}\n",
    "            <tr>\n",
    "                <td>{{ s.timestamp }}</td>\n",
    "                <td>SESSION</td>\n",
    "                <td>{{ s.user_agent[:30] }}...</td>\n",
    "            </tr>\n",
    "            {% endfor %}\n",
    "        </table>\n",
    "    </div>\n",
    "\n",
    "    <script>\n",
    "        // Parse data passed from Flask\n",
    "        const chartData = {{ chart_data | tojson }};\n",
    "\n",
    "        // 1. TOP QUERIES BAR CHART\n",
    "        const ctxQuery = document.getElementById('queryChart').getContext('2d');\n",
    "        new Chart(ctxQuery, {\n",
    "            type: 'bar',\n",
    "            data: {\n",
    "                labels: chartData.q_labels,\n",
    "                datasets: [{\n",
    "                    label: 'Search Count',\n",
    "                    data: chartData.q_data,\n",
    "                    backgroundColor: 'rgba(54, 162, 235, 0.6)',\n",
    "                    borderColor: 'rgba(54, 162, 235, 1)',\n",
    "                    borderWidth: 1\n",
    "                }]\n",
    "            },\n",
    "            options: {\n",
    "                responsive: true,\n",
    "                maintainAspectRatio: false,\n",
    "                scales: { y: { beginAtZero: true, ticks: { stepSize: 1 } } }\n",
    "            }\n",
    "        });\n",
    "\n",
    "        // 2. ACTIVITY LINE CHART\n",
    "        const ctxTime = document.getElementById('timeChart').getContext('2d');\n",
    "        new Chart(ctxTime, {\n",
    "            type: 'line',\n",
    "            data: {\n",
    "                labels: chartData.time_labels,\n",
    "                datasets: [\n",
    "                    {\n",
    "                        label: 'Searches',\n",
    "                        data: chartData.time_searches,\n",
    "                        borderColor: 'rgba(54, 162, 235, 1)',\n",
    "                        backgroundColor: 'rgba(54, 162, 235, 0.2)',\n",
    "                        tension: 0.3,\n",
    "                        fill: true\n",
    "                    },\n",
    "                    {\n",
    "                        label: 'Clicks',\n",
    "                        data: chartData.time_clicks,\n",
    "                        borderColor: 'rgba(255, 99, 132, 1)',\n",
    "                        backgroundColor: 'rgba(255, 99, 132, 0.2)',\n",
    "                        tension: 0.3,\n",
    "                        fill: true\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            options: {\n",
    "                responsive: true,\n",
    "                maintainAspectRatio: false,\n",
    "                scales: { y: { beginAtZero: true, ticks: { stepSize: 1 } } }\n",
    "            }\n",
    "        });\n",
    "    </script>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f502c560",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def prepare_chart_data(analytics):\n",
    "    \"\"\"\n",
    "    Prepares data for Chart.js.\n",
    "    1. Top Queries: Extract labels and counts.\n",
    "    2. Activity Timeline: Align searches and clicks by minute.\n",
    "    \"\"\"\n",
    "    # --- Chart 1: Top Queries ---\n",
    "    # analytics.get_stats() returns top_queries as [('query', count), ...]\n",
    "    stats = analytics.get_stats()\n",
    "    top_queries = stats['top_queries']\n",
    "    q_labels = [q[0] for q in top_queries]\n",
    "    q_data = [q[1] for q in top_queries]\n",
    "\n",
    "    # --- Chart 2: Activity Over Time ---\n",
    "    # We need to group events by minute to plot them on a shared timeline\n",
    "    activity_buckets = defaultdict(lambda: {'searches': 0, 'clicks': 0})\n",
    "    \n",
    "    # Process Searches\n",
    "    for s in analytics.searches:\n",
    "        # ISO format is YYYY-MM-DDTHH:MM:SS... -> Slice to minute YYYY-MM-DDTHH:MM\n",
    "        time_bucket = s['timestamp'][:16].replace('T', ' ')\n",
    "        activity_buckets[time_bucket]['searches'] += 1\n",
    "        \n",
    "    # Process Clicks\n",
    "    for c in analytics.clicks:\n",
    "        time_bucket = c['timestamp'][:16].replace('T', ' ')\n",
    "        activity_buckets[time_bucket]['clicks'] += 1\n",
    "\n",
    "    # Sort by time so the line chart moves left to right\n",
    "    sorted_times = sorted(activity_buckets.keys())\n",
    "    \n",
    "    # Create aligned arrays\n",
    "    timeline_labels = sorted_times\n",
    "    timeline_searches = [activity_buckets[t]['searches'] for t in sorted_times]\n",
    "    timeline_clicks = [activity_buckets[t]['clicks'] for t in sorted_times]\n",
    "\n",
    "    return {\n",
    "        'q_labels': q_labels,\n",
    "        'q_data': q_data,\n",
    "        'time_labels': timeline_labels,\n",
    "        'time_searches': timeline_searches,\n",
    "        'time_clicks': timeline_clicks\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0dcb8aa0-1bad-4a6b-89c0-7e5bb58bdc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Flask Server...\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://10.80.134.73:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [03/Dec/2025 14:56:16] \"GET /search?q=men+jeans HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Dec/2025 14:56:25] \"GET /product/c4c281e3-adc5-45d8-b935-a9351e3cf625?sid=f695c40c-8a6b-4c9f-9489-8eeb63f482b3&qid=12c216da-fdc3-477a-9395-9532df0ac829&rank=1 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Dec/2025 14:56:28] \"POST /track_dwell HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Dec/2025 14:56:30] \"GET /dashboard HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.before_request\n",
    "def ensure_session():\n",
    "    # Simple session tracking via cookie logic (simplified)\n",
    "    pass\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    # Start a session\n",
    "    sid = analytics.track_session(request.headers.get('User-Agent'), request.remote_addr)\n",
    "    return render_template_string(HOME_TEMPLATE)\n",
    "\n",
    "@app.route('/search')\n",
    "def search():\n",
    "    query = request.args.get('q', '')\n",
    "    \n",
    "    # 1. Analytics: Get or Create Session\n",
    "    sid = analytics.track_session(request.headers.get('User-Agent'), request.remote_addr)\n",
    "    \n",
    "    # 2. Search Engine\n",
    "    results = engine.search(query, k=15)\n",
    "    \n",
    "    # 3. Analytics: Track Search\n",
    "    search_id = analytics.track_search(sid, query, len(results))\n",
    "    \n",
    "    # 4. RAG: Generate Summary\n",
    "    summary = rag.generate_summary(query, results)\n",
    "    \n",
    "    return render_template_string(\n",
    "        RESULTS_TEMPLATE, \n",
    "        query=query, \n",
    "        results=results, \n",
    "        summary=summary,\n",
    "        session_id=sid,\n",
    "        search_id=search_id\n",
    "    )\n",
    "\n",
    "@app.route('/product/<uid>')\n",
    "def product_detail(uid):\n",
    "    # Retrieve query params for analytics\n",
    "    sid = request.args.get('sid', 'unknown')\n",
    "    qid = request.args.get('qid', 'unknown')\n",
    "    rank = request.args.get('rank', 0)\n",
    "    \n",
    "    # 1. Analytics: Track Click\n",
    "    analytics.track_click(sid, qid, uid, rank)\n",
    "    \n",
    "    # 2. Find Document\n",
    "    doc = engine.df[engine.df['uid'] == uid].iloc[0].to_dict()\n",
    "    \n",
    "    return render_template_string(\n",
    "        DETAILS_TEMPLATE, \n",
    "        doc=doc, \n",
    "        session_id=sid\n",
    "    )\n",
    "\n",
    "@app.route('/track_dwell', methods=['POST'])\n",
    "def track_dwell():\n",
    "    # Endpoint for Beacon API\n",
    "    data = json.loads(request.data)\n",
    "    analytics.update_dwell_time(data['session_id'], data['doc_uid'], data['duration'])\n",
    "    return jsonify({\"status\": \"success\"})\n",
    "\n",
    "@app.route('/dashboard')\n",
    "def dashboard():\n",
    "    stats = analytics.get_stats()\n",
    "    \n",
    "    # New: Prepare data for charts\n",
    "    chart_data = prepare_chart_data(analytics)\n",
    "    \n",
    "    return render_template_string(\n",
    "        DASHBOARD_TEMPLATE, \n",
    "        stats=stats,\n",
    "        chart_data=chart_data # Pass this new variable\n",
    "    )\n",
    "if __name__ == '__main__':\n",
    "    # Run the server\n",
    "    print(\"Starting Flask Server...\")\n",
    "    app.run(host='0.0.0.0', port=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
